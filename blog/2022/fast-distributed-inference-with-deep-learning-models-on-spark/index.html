<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Fast-distributed inference with Deep Learning models on Spark | Romulo Drumond</title> <meta name="author" content="Romulo B. P. Drumond"/> <meta name="description" content="Learn how to efficiently distribute inference load on Spark"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon-32x32.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://romulodrumond.com/blog/2022/fast-distributed-inference-with-deep-learning-models-on-spark/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Romulo Drumond</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fast-distributed inference with Deep Learning models on Spark</h1> <p class="post-meta">November 7, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/spark"> <i class="fas fa-hashtag fa-sm"></i> Spark</a>   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> Deep Learning</a>   </p> </header> <article class="post-content"> <p>Machine learning (ML) models are getting bigger and bigger and <a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener noreferrer">this trend doesn’t seem to be over</a>. As a result of that, has become mainstream in the industry the use of big models, usually referred to as Deep Learning (DL) models. Although a few companies do have enough data and processing power to train DL from scratch, a common practice is to leverage these big models to produce embeddings, which are a kind of vectorial representation of “unstructured” data as text, images, audio, and videos. The embeddings may be the final product/objective of these models as the <a href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener noreferrer">Universal Sentence Encoder (USE)</a> or a by-product of getting the output of the internal layers of a model (eg. <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a> internal layers). See Figure 1.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings-1400.webp"></source> <img src="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 1 - Embeddings generation with USE (the default output of the model) and with BERT (the embeddings are extracted from an internal layer or a combination of them).</figcaption> </figure> <p>Note: See <a href="http://jalammar.github.io/illustrated-bert/#:~:text=BERT%20for%20feature%20extraction" target="_blank" rel="noopener noreferrer">this</a> to know more about using BERT to generate embeddings.</p> <p>The use of these “embedders” is a good practice as it is a way to leverage pre-trained models to produce tabular data to later be fed in traditional ML techniques (eg. logistic regressions, multilayer perceptrons etc). Companies can then leverage their unstructured data to be used for multiple teams and projects, some companies also started using <a href="https://zilliz.com/learn/what-is-vector-database" target="_blank" rel="noopener noreferrer">vector databases</a> (if you prefer the video format, <a href="https://www.youtube.com/watch?v=g2bNHLeKlAg&amp;ab_channel=SouthernDataScienceConference" target="_blank" rel="noopener noreferrer">this is a good high-level intro</a>) as a way to store these embeddings and have easy use of them in search scenarios.</p> <p>In this context, some companies do the batch generation of these embeddings on big data, on a Spark cluster, as in the company I work on.</p> <p>In a nutshell, <a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a> is a distributed computing engine that is run on clusters. The architecture consists of a Driver node that sends jobs to, possibly, multiple Worker nodes.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview-1400.webp"></source> <img src="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"><small><i>Source: https://spark.apache.org/docs/latest/cluster-overview.html</i></small><br> Figure 1 - Spark Cluster overview.</figcaption> </figure> </div> <p>As DL models usually involve many parallel operations (as matrix multiplications), a GPU can be multiple times faster than a multicore CPU to run inference/embedding generation. In this scenario, our team used a cluster comprised of multiple Worker nodes, each one with a single GPU (see Figure 2).</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration-1400.webp"></source> <img src="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 2 - Overview of cluster used.</figcaption> </figure> </div> <p>With the right abstractions, an efficient <a href="https://spark.apache.org/docs/latest/api/python/" target="_blank" rel="noopener noreferrer">PySpark</a> code would be something along these lines:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td> <td class="code"><pre><span class="k">def</span> <span class="nf">generate_embeddings</span><span class="p">(</span><span class="n">embeder_model</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="n">broacasted_embeddings_model</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">broadcast</span><span class="p">(</span><span class="n">embeder_model</span><span class="p">)</span>

    <span class="nd">@pandas_udf</span><span class="p">(</span><span class="n">returnType</span><span class="o">=</span><span class="nc">ArrayType</span><span class="p">(</span><span class="nc">FloatType</span><span class="p">()))</span>
    <span class="k">def</span> <span class="nf">batch_embed</span><span class="p">(</span><span class="n">dataset</span><span class="p">:</span> <span class="n">pandas</span><span class="p">.</span><span class="n">Series</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pandas</span><span class="p">.</span><span class="n">Series</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">broacasted_embeddings_model</span><span class="p">.</span><span class="n">value</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">toGPU</span><span class="p">()</span>

        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        
        <span class="n">embeddings_cpu</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">embeddings</span> <span class="k">else</span> <span class="p">[]</span>

        <span class="k">return</span> <span class="n">pandas</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">embeddings_cpu</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">dataframe</span><span class="p">.</span><span class="nf">withColumn</span><span class="p">(</span><span class="n">EMBEDDINGS_COLUMN_NAME</span><span class="p">,</span> <span class="nf">batch_embed</span><span class="p">(</span><span class="n">dataframe</span><span class="p">[</span><span class="n">DATA_COLUMN_NAME</span><span class="p">]))</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now for you to understand this pseudo-code we have some important notes:</p> <ul> <li> <p><strong>Broadcast of the model</strong><br> As each worker node needs the model, it has to be sent beforehand. Line 2 broadcasts the model from the driver node and in line 6 a worker node gets the python object back.</p> </li> <li> <p><strong>Why pandas UDF instead of Spark UDF?</strong><br> Traditional Spark UDFs run row by row of the data frame while pandas UDFs run on batches of the data (multiple rows at once). Trying to run a DL model for each row separately would add unnecessary GPU to CPU communication and wouldn’t leverage GPU’s parallel capabilities.</p> </li> </ul> <p>Other important params of your code and Spark cluster might be:</p> <ul> <li> <strong>Apache Arrow batch size</strong>: you might need to reduce the default 10,000 value of <code class="language-plaintext highlighter-rouge">spark.sql.execution.arrow.maxRecordsPerBatch</code> if your data is too big (take a look <a href="https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#setting-arrow-batch-size" target="_blank" rel="noopener noreferrer">here</a>);</li> <li> <strong>Data batch size for the GPU</strong>: although choosing a power of 2 (eg. 32, 64, 128…) doesn’t seem to have a real impact, it may help you reduce the search space of this parameter. The clear rule is to try to maximize your GPU memory use without running into Out Of Memory (OOM) errors;</li> <li> <strong>GPU choice</strong>: although the number of cores and memory available widely varies between GPUs, your choice may be restricted by what is available from your vendor and the size of the model used.</li> </ul> <p>A final important note is that even if nodes with GPU are more expensive than CPU only, the final cost for running batch prediction (\($ = hourlyCost * hoursNeeded\)) is usually in favor of GPU clusters for DL models.</p> <p>That’s it! Feel free to reach out if you have doubts or some feedback to give.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Romulo B. P. Drumond. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>