<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Navigating the Web with Python: Insights into Scraping and Automation Tools | Romulo Drumond</title> <meta name="author" content="Romulo B. P. Drumond"/> <meta name="description" content="Because manually browsing the web is so 1990s."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon-32x32.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://romulodrumond.com/blog/2023/navigating-the-web-with-python/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Romulo Drumond</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Navigating the Web with Python: Insights into Scraping and Automation Tools</h1> <p class="post-meta">December 6, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> Python</a>   <a href="/blog/tag/web-scraping"> <i class="fas fa-hashtag fa-sm"></i> Web scraping</a>   <a href="/blog/tag/web-automation"> <i class="fas fa-hashtag fa-sm"></i> Web automation</a>   </p> </header> <article class="post-content"> <ul> <li><a href="#1-introduction">1. Introduction</a></li> <li> <a href="#2-tool-overview">2. Tool overview</a> <ul> <li><a href="#21-beautiful-soup">2.1. Beautiful Soup</a></li> <li><a href="#22-selenium">2.2. Selenium</a></li> <li><a href="#23-scrapy">2.3. Scrapy</a></li> </ul> </li> <li><a href="#3-choosing-the-right-tool-for-the-job">3. Choosing the right tool for the job</a></li> <li><a href="#4-tldr">4. TLDR</a></li> <li><a href="#appendix-a-exploring-additional-webpage-rendering-tools">Appendix A: Exploring Additional Webpage Rendering Tools</a></li> <li><a href="#appendix-b-waiting-for-browser-rendering-sucks">Appendix B: Waiting for browser rendering sucks</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>No developer can afford not to know or interact with web technologies. From APIs to network security protocols that one needs to attend for the company, a developer, and even ‘data’ people, end up interacting with HTTP requests, VPNs, and network boundaries. But fancy jargon aside this post is not going to be either about serving on the web, developing a web application on Django or Flask, or exposing an API with FastAPI, we gonna talk about being a client, a consumer, being ‘served by the web’.</p> <p>When do we act as a web client outside of traditional web browsing? This often occurs in the realms of <strong>web scraping</strong> and <strong>web automation</strong>. Web scraping involves programmatically extracting data from websites, while web automation refers to automating web-based tasks, sometimes known as <a href="https://en.wikipedia.org/wiki/Robotic_process_automation" target="_blank" rel="noopener noreferrer">Robotic Process Automation (RPA)</a>. These practices are especially relevant when dealing with legacy systems or in scenarios where direct user interaction is restricted. In this context, I want to discuss three widely used tools intended for these applications: <strong>Beautiful Soup</strong>, <strong>Selenium</strong>, and <strong>Scrapy</strong>. Each of these tools offers unique features and capabilities, making them go-to choices for web scraping and automation tasks.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy-1400.webp"></source> <img src="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 1 - The Web Warriors Civil War.</figcaption> </figure> </div> <style>@media(min-width:768px){.img-fluid{width:50%}}@media(max-width:767px){.img-fluid{width:100%}}</style> <h1 id="2-tool-overview">2. Tool overview</h1> <h2 id="21-beautiful-soup">2.1. Beautiful Soup</h2> <p>Is the simplest of the three, it is basically an HTML/XML parser that provides a more user-friendly interface for iterating, searching, and parsing the HTML document tree.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer-1400.webp"></source> <img src="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 2 - Beautiful Soup serving a delicious Python object to a developer.</figcaption> </figure> </div> <p>For fetching/acquiring the data to be parsed by Beautiful Soup you will need other tools such as Python’s <code class="language-plaintext highlighter-rouge">requests</code> built-in library to make the HTTP requests to the web services. Given its parsing nature, Beautiful Soup is a library used more for web scraping when someone wants to abstract a bit the difficulties of understanding <a href="https://www.w3schools.com/xml/xpath_syntax.asp" target="_blank" rel="noopener noreferrer"><em>xpaths</em></a> and <a href="https://www.w3schools.com/cssref/css_selectors.php" target="_blank" rel="noopener noreferrer"><em>css selectors</em></a>. A common example of using this library can be found below.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td> <td class="code"><pre><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="c1"># URL of the webpage to scrape
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://books.toscrape.com/</span><span class="sh">'</span>

<span class="c1"># Send HTTP request to the URL
</span><span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># Check if the request was successful
</span><span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="c1"># Parse the content of the response
</span>    <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">html.parser</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Find all h3 tags (which contain book titles)
</span>    <span class="n">book_titles</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Extract the titles and write to a file
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">book_titles.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">book_titles</span><span class="p">):</span>
            <span class="n">book_title</span> <span class="o">=</span> <span class="n">title</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">]</span>
            <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">book_title</span><span class="si">}</span><span class="sh">"</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
            <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">line</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Failed to retrieve the webpage. Status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1: A Light in the Attic
2: Tipping the Velvet
3: Soumission
4: Sharp Objects
5: Sapiens: A Brief History of Humankind
...
</code></pre></div></div> <h2 id="22-selenium">2.2. Selenium</h2> <p>Selenium is kind of a big monster that has made a name for itself among web developers, primarily because it was originally developed for automated software testing of web applications. Its versatility is further showcased by its support for multiple programming languages, including Java, Python, C#, Ruby, JavaScript, and Kotlin. Unlike libraries that are limited to parsing HTML, Selenium offers users the ability to control a full-fledged browser, automating a wide range of tasks across <a href="https://www.selenium.dev/documentation/webdriver/browsers/" target="_blank" rel="noopener noreferrer">several supported browsers</a>.</p> <div style="text-align: center"> <figure> <picture> <img src="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_automation.gif" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Gif 1 - Example of Selenium controlling a Firefox browser.</figcaption> </figure> </div> <p>As you may have already noticed, Selenium is pretty friendly with the idea of automating web tasks, i.e. RPA, as it can click on buttons, and scroll the page, just like simulating a human interaction with the browser. But Selenium is also used for web scraping sometimes. <em>Why is that?</em> You may ask. Because scraping JavaScript-heavy (JS-heavy) websites is not as straightforward as doing HTTP requests. For example:</p> <ol> <li>Browse to the <a href="https://www.openweb.com" target="_blank" rel="noopener noreferrer">www.openweb.com</a> webpage</li> <li>Open the developer tools (F12 usually);</li> <li>Disable the JavaScript. You can look for the check box or press ctrl + shift + P, then “disable javascript” on the search box;</li> <li>Update the page (F5);</li> </ol> <p>You will notice that things don’t look the same. You may also make an HTTP request to this page and check the returned HTML doesn’t have all the information you can see on the HTML of your browser.</p> <p>Now you see that things are not what they look like. Web pages on JS-heavy sites may appear static, but there’s more than meets the eye. Often, these pages dynamically load content, initiating new requests for assets after the initial page render. This is due to the site’s JavaScript, which fetches additional data and updates the display in real time. For those new to web development, a practical way to observe this is by opening the ‘network’ tab in your browser’s developer tools. There, you can monitor the various requests your browser makes to fully render the page.</p> <p>Because of these types of sites Selenium shines in the scrappy activity, as it is a full browser, it automatically runs all the JS that is needed to render the page for you, landing the data you want to acquire directly on the modified page HTML.</p> <p><em>Wait! For those JS-heavy sites, I can only hope to wait for a tool like Selenium to render the page?</em></p> <p>Actually no, you can replicate the requests you found on the ‘network’ tab on your Python code, using Selenium just makes the job pretty easy, despite adding a lot of latency and asking for more computing resources.</p> <h2 id="23-scrapy">2.3. Scrapy</h2> <p>Scrapy is a bit different from Beautiful Soup and Selenium. Scrapy is a framework, not a library. Instead of writing code that uses Scrapy, you write code that the Scrapy framework will use. For example, if you want to craw and dump a whole website you just need to:</p> <ol> <li>Run the scrappy create command: <code class="language-plaintext highlighter-rouge">scrapy startproject &lt;project-name&gt;</code>;</li> <li>Put a code similar to this one in the spider:</li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td> <td class="code"><pre><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">scrapy</span>
<span class="kn">from</span> <span class="n">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="n">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">ExampleCrawlSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">examplecrawlspider</span><span class="sh">'</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">example.com</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Replace with the target domain
</span>    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">http://example.com</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Replace with the starting URL
</span>
    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c1"># Rule to follow links (LinkExtractor), calling the defined 
</span>        <span class="c1"># 'parse_item' method
</span>        <span class="nc">Rule</span><span class="p">(</span><span class="nc">LinkExtractor</span><span class="p">(),</span> <span class="n">callback</span><span class="o">=</span><span class="sh">'</span><span class="s">parse_item</span><span class="sh">'</span><span class="p">,</span> <span class="n">follow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># Extract the entire content of the &lt;body&gt; tag from the response
</span>        <span class="n">page_content</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="sh">'</span><span class="s">body</span><span class="sh">'</span><span class="p">).</span><span class="nf">get</span><span class="p">()</span>

        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">dumped_pages/</span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">.html</span><span class="sh">'</span>
        <span class="k">if</span> <span class="n">filename</span> <span class="o">==</span> <span class="sh">'</span><span class="s">dumped_pages/.html</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="sh">'</span><span class="s">dumped_pages/index.html</span><span class="sh">'</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="sh">'</span><span class="s">dumped_pages</span><span class="sh">'</span><span class="p">):</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">dumped_pages</span><span class="sh">'</span><span class="p">)</span>

        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">page_content</span><span class="p">.</span><span class="nf">encode</span><span class="p">())</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Saved file </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Note: You might need to change some configurations before running the spider</p> <p>As you may imagine, by its name, Scrappy is the go-to framework for large-scale scraping and crawling. Did you start scraping with Python and now want to put things async with aiohttp? Scrapy is async by default. Need to add data validation to the extracted information? Need to do automatic throttling? Banned detection? Add/swap middleware? Distributed multi-node crawling? <strong>Scrapy can do it all</strong>.</p> <p>Don’t get me wrong, I know that using a framework like Scrapy adds complexity and new things to learn, but if you think you can go a long way with Python’s request + Beautiful Soup is because you can’t see how far away you would if using Scrapy.</p> <p>Note: <a href="https://www.youtube.com/@JohnWatsonRooney" target="_blank" rel="noopener noreferrer">John Watson Rooney</a> is a pretty good channel with tutorials about Scrapy and other web scraping tools.</p> <h1 id="3-choosing-the-right-tool-for-the-job">3. Choosing the right tool for the job</h1> <p>Deciding on the right tool can be overwhelming. To simplify the process, consider two main factors: the scope of your project — including the number of sites, requests, operations, or items involved — and your primary goal, which might range from web testing to simple or extensive web scraping. Wondering how to choose between the three? Below is a table that summarizes their key attributes and uses:</p> <table> <thead> <tr> <th><strong>Feature/Tool</strong></th> <th><strong>Beautiful Soup</strong></th> <th><strong>Selenium</strong></th> <th><strong>Scrapy</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Primary Use</strong></td> <td>HTML/XML Parsing</td> <td>Web Automation</td> <td>Large-Scale Web Scraping</td> </tr> <tr> <td><strong>Ease of Use</strong></td> <td>Easy for beginners</td> <td>Moderate</td> <td>Steeper learning curve</td> </tr> <tr> <td><strong>Performance</strong></td> <td>Fast for simple tasks</td> <td>Slower, resource-intensive</td> <td>Fast, efficient for large tasks</td> </tr> <tr> <td><strong>JS Heavy Sites</strong></td> <td>Limited (needs extra handling)</td> <td>Excellent (renders JS)</td> <td>Good (with some workarounds)</td> </tr> <tr> <td><strong>Scalability</strong></td> <td>Ideal for small projects</td> <td>Not ideal for large-scale scraping</td> <td>Highly scalable</td> </tr> <tr> <td><strong>Integration</strong></td> <td>Works with Python requests</td> <td>Full-fledged browser control</td> <td>Extensive framework capabilities</td> </tr> <tr> <td><strong>Best For</strong></td> <td>Quick scraping tasks, learning basics</td> <td>Automating browser tasks, handling JS-heavy sites</td> <td>Advanced web scraping, large-scale projects</td> </tr> </tbody> </table> <p><br></p> <p>So, in the end, if you are looking for automation go to Selenium and if you are looking for web scraping go for Beautiful Soup or/and Scrapy (nothing prohibits you from using both).</p> <p><em>Wait, and what about the JS-heavy sites?</em></p> <p>Yeah, Selenium makes things easy on this front, but it’s really slow and hardware-hungry (it’s a browser in the end, everybody knows that they are becoming almost as containerized operational systems (OS)). If you need to cut costs and be blazing fast you need to go raw and do the extra request by yourself (inspect the website with the dev tools and discover what request gets the data you want) and replicate it in Beautiful Soup/Scrapy. <strong>That’s a no-brainer: when scraping, rendering is helpful for the human eye, but remember that you don’t want to render things, what you really want is the data.</strong></p> <h1 id="4-tldr">4. TLDR</h1> <p><em>What to choose if I’m starting and don’t need to do a lot of requests?</em></p> <ul> <li>Beautiful Soup.</li> <li> <em>But what if the site is JS-heavy?</em> <ul> <li>Selenium or Playwright.</li> </ul> </li> </ul> <p><em>What if I do need a lot of requests?</em> (being I a beginner or not)</p> <ul> <li>Go for Scrapy (if you are a beginner, be brave, the effort will pay off).</li> </ul> <p><em>If I want to automate web interactions, maybe RPA?</em></p> <ul> <li>Selenium/Playwright.</li> <li>You might go raw with BeatitulSoup/Scrapy, translating click and API calls, it depends on your project’s requirements. Going raw will ask you to understand the web better (API calls, headers, sessions, cookies).</li> </ul> <h1 id="appendix-a-exploring-additional-webpage-rendering-tools">Appendix A: Exploring Additional Webpage Rendering Tools</h1> <p>While I’ve highlighted the use of raw HTTP requests and Selenium, it’s worth noting that the ecosystem of tools for webpage rendering is rich with options. You can explore tools like <a href="https://playwright.dev/python/docs/api/class-playwright" target="_blank" rel="noopener noreferrer">Playwright</a>, which offers browser automation capabilities such as Selenium, or <a href="https://github.com/scrapy-plugins/scrapy-splash" target="_blank" rel="noopener noreferrer">Splash</a>, a lightweight browser render service that integrates well with Scrapy. For those seeking a more managed solution, services like <a href="https://docs.zyte.com/zyte-api/usage/http.html#html-and-browser-html" target="_blank" rel="noopener noreferrer">Zyte</a> can render pages on your behalf.</p> <p>Transitioning to raw HTTP requests can be a smooth process. Here’s an approach to deepen your web scraping techniques progressively:</p> <ol> <li>Begin with Python’s <code class="language-plaintext highlighter-rouge">requests</code> library in pair with Beautiful Soup. This will introduce you to the basics of web scraping and HTML parsing.</li> <li>Move on to browser-based scraping with Selenium to handle JavaScript-heavy websites. You can continue to use Beautiful Soup to parse the HTML content fetched.</li> <li>Integrate Splash with Scrapy if you’re working within the Scrapy framework and need to render JavaScript without a full-fledged browser.</li> <li>Consider using Zyte’s rendering service for a server-side solution that can scale and handle complex scraping tasks without the need to manage browsers or proxies yourself.</li> <li>When ready, make the leap to Scrapy for an all-in-one framework experience, providing you with asynchronous processing capabilities and a wide range of built-in features for large-scale web scraping projects.</li> </ol> <p>By pacing your learning this way, you’ll build a solid foundation in web scraping and automation, allowing you to choose the right tool for each job with more confidence.</p> <h1 id="appendix-b-waiting-for-browser-rendering-sucks">Appendix B: Waiting for browser rendering sucks</h1> <p>Browser rendering delays are a common pain point in web scraping, particularly on JavaScript-rich sites. Playwright claims to handle these delays more adeptly than Selenium, though I’ve yet to thoroughly test this myself. From experience, Selenium’s built-in wait functions often fall short on such sites. I once had to engineer custom wait functions to deal with a particularly tricky site, which required me to measured max image dissimilarity between sequential screenshots to determine when the page had fully loaded.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Romulo B. P. Drumond. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>