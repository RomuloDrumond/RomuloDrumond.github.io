<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://romulodrumond.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://romulodrumond.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-28T00:01:15+00:00</updated><id>https://romulodrumond.com/feed.xml</id><title type="html">Romulo Drumond</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Unspoken Ineffectiveness of AI Agents</title><link href="https://romulodrumond.com/blog/2025/ineffectiveness-of-ai-agents/" rel="alternate" type="text/html" title="The Unspoken Ineffectiveness of AI Agents"/><published>2025-03-27T04:00:00+00:00</published><updated>2025-03-27T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2025/ineffectiveness-of-ai-agents</id><content type="html" xml:base="https://romulodrumond.com/blog/2025/ineffectiveness-of-ai-agents/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a></li> <li><a href="#2-you-put-ai-agents-in-production-and-then">2. You put AI Agents in production, and then‚Ä¶</a></li> <li><a href="#3-the-autonomy-landscape">3. The Autonomy Landscape</a></li> <li><a href="#4-conclusion-agi-when">4. Conclusion: AGI when??</a></li> <li><a href="#appendix-a-why-do-ai-multi-agent-frameworks-like-crewai-and-autogen-work">Appendix A: Why do AI multi-agent frameworks like CrewAI and AutoGen work?</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>I feel that this blog post might be almost too late‚Ä¶ <em>Why?</em> Things are moving too fast! We now have reasoning models, and in my experience, <strong>Claude 3.7 - Thinking</strong> may have increased the ratio between (<em>this mtfk knows stuff, let him cook</em>) and (the unbearable sadness of seeing it be the dumbest dude) by an order of magnitude. So, by the time you finish reading this blog post, it might already be outdated.</p> <p>Now, getting over my little rant, one may start to focus on the title of this blog post and ask oneself:</p> <p style="text-align: center;"><strong><i><span style="color: #FF3636;"> "But, what do AI Agents even mean?" </span></i></strong></p> <p>And that‚Äôs a great question! Unfortunately, there is no straightforward answer. The best I can offer is: it depends on who you are and who you are talking to. So, if you find yourself in one of these categories:</p> <div> <ul> <li><strong>Selling something to someone</strong> (startup founder?): an AI Agent is anything that <strong>seems</strong> intelligent or automated; it could even be people in the background manually doing tasks.</li> <li><strong>You code / Have some Software Engineering experience</strong>: an AI Agent can be any LLM-based or enhanced application, usually something that wasn‚Äôt possible before with ‚Äúclassical‚Äù software.</li> <li><strong>You are deep in GenAI and LLMs</strong>: whether you‚Äôre a hobbyist or a seasoned ML engineer, for you, an AI Agent is not just about LLMs but also about the degree of autonomy they possess. An AI Agent is an LLM-based application where the LLM creates and adapts its workflow or graph. This aligns with the <a href="https://blog.langchain.dev/what-is-an-agent/">definition from LangChain/graph folks</a>.</li> <li> <details> <summary><b>You are an RL practitioner</b></summary> <p>For you, things can be even more interesting. You might even argue that ChatGPT is already an agent!</p> <p><img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/chatbots-are-rl-agents.png" alt=" Diagram on how chatbots can be viewed as RL agents" width="50%" style=" display: block; margin-left: auto; margin-right: auto;"/></p> <p style="text-align: center; margin-top: 0.5em;">Figure 1: Diagram showing LLMs as RL agents</p> <p>You can say that chatbots:</p> <ul> <li><strong>Perceive the environment</strong>: they consider the message history between them and you;</li> <li><strong>Act in the environment, changing it</strong>: if you don't think so, just ask GPT 4.5 this: <em> "What important truth do you think most people refuse to acknowledge, and why do you think they avoid facing it?" </em>. If you feel changed or moved, remember that you are a part of the world observation space.</li> </ul> </details> </li> </ul> </div> <p>In my opinion, the best definition is the one for software developers: <strong>AI Agents are LLM-based or enhanced applications</strong>. This broad definition helps us, as a society, understand each other better, from ML researchers to non-tech people.</p> <div style="text-align: center;"> <img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/ai-agents-by-the-author.png" alt="AI Agents by the author" style="max-width: 50%; height: auto;"/> </div> <p style="text-align: center; margin-top: 0.5em;">Figure 2: AI Agents bar, as seen by the author</p> <h1 id="2-you-put-ai-agents-in-production-and-then">2. You put AI Agents in production, and then‚Ä¶</h1> <p>‚Ä¶ they suck, hahahahaha ü§£. Yeah, man, <strong>LLMs suck</strong>; they are not all-mighty as many of us feel at the beginning. While the ‚Äú<em>vibes</em>‚Äù might be strong at first, nothing kills the <em>vibes</em> quite like real-world interaction.</p> <p>So, <em>what can we do then?</em> The answer is to take away their freedom. Don‚Äôt expect them to handle all the reasoning and planning on their own, nor should you expect them always to write runnable code. Instead, give the LLM tasks that are as simple as possible.</p> <div style="text-align: center;"> <img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/first-ai-agents.png" alt="Devs fighting LLM bugs" style="max-width: 50%; height: auto;"/> </div> <p style="text-align: center; margin-top: 0.5em;">Figure 3: Developers dealing with LLMs on a typical Friday afternoon</p> <p>As you gain more experience with AI Agents in production, it becomes increasingly clear how to work with LLMs effectively: you reduce their autonomy and develop the AI Agent to resemble more traditional software. This realization might prompt you to consider‚Ä¶</p> <h1 id="3-the-autonomy-landscape">3. The Autonomy Landscape</h1> <p>Alright, so LLMs are now a part of the ~<em>modern</em>~ software stack and are being used everywhere. But how should you use them? The answer, of course, is that it <strong>depends</strong>. The key lies in finding the balance between <strong><span style="color: green;">flexibility/autonomy</span></strong> and <strong><span style="color: darkblue;">control/reliability</span></strong>.</p> <div style="text-align: center;"> <img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/the-autonomy-landscape.png" alt="The AI Agents autonomy landscape or tradeoff" style="max-width: 80%; height: auto;"/> </div> <p style="text-align: center; margin-top: 0.5em;">Figure 4: The autonomy landscape (or trade-off) for AI Agents</p> <p>I hope Figure 4 clarifies this point: the more autonomy you grant to the LLM, the further to the right you find yourself on the graph. However, with increased autonomy comes unpredictability.</p> <p>Therefore, your AI Agent solution can fall into one of these categories:</p> <ol> <li><strong>Code</strong>: This isn‚Äôt really an AI Agent; your solution is traditional code or software. It‚Äôs predictable, testable, and well-known to humankind.</li> <li> <p><strong>LLM Call/Chain</strong>: Your solution is almost traditional software, but now and then, you call an LLM, perhaps to extract or clean data/text. Your chain could even consist of a simple sequence of discrete LLM calls, as shown in Figure 5.</p> <p><img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/example-llm-workflow.png" alt="Example LLM workflow" width="60%" style="display: block; margin-left: auto; margin-right: auto;"/></p> <p style="text-align: center; margin-top: 0.5em;">Figure 5: Example LLM Chain for generating research insights</p> </li> <li> <p><strong>Router</strong>: Here, your solution becomes a bit more complex than a simple chain. You introduce branching or a router LLM that decides which next step to take based on the input. The next step could even involve other workflows or agents! This approach remains reasonably controlled while offering greater flexibility to handle various types of inputs and scenarios.</p> <p><img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/llm-as-router.png" alt="Example LLM as a router" width="40%" style="display: block; margin-left: auto; margin-right: auto;"/></p> <p style="text-align: center; margin-top: 0.5em;">Figure 6: Graphical representation of an LLM as a router</p> </li> <li><strong>Full Autonomy/AI Agents for real</strong>: In this scenario, you allow the LLM to decide the workflow. Questions like, <i> ‚ÄúShould I move forward with the document or ask for a second review?‚Äù </i> or <i> ‚ÄúShould I ask for human feedback?‚Äù </i> come into play. In these cases, you hand off the heavy lifting to the agent: it needs to reason, plan, and adapt‚Ä¶ <abbr title="Relax dude, I know AGI has arrived and we live in a utopia, I wrote this on 2025-03-27, just chill...">Do I really need to say that this is asking too much of these models?</abbr></li> </ol> <p>The truth is, the closer your application resembles traditional software, the better your outcomes. You would have:</p> <ol> <li><strong>Better predictability</strong>: Code is, in most cases, quite deterministic, and humans can follow its branching behavior.</li> <li><strong>A longer history of coding</strong>: It‚Äôs a well-established technology, making debugging and troubleshooting straightforward.</li> <li><strong>Security and compliance</strong>: Code can be audited line by line for vulnerabilities and regulatory compliance.</li> </ol> <p>Don‚Äôt get me wrong; it‚Äôs a marvel to see an AI Agent make the right decisions and navigate the right steps in a complex workflow. However, this doesn‚Äôt happen as frequently as we might wish. Initially, you should go on an AI Agent project by granting complete autonomy to the LLM. Still, soon, you will discover that providing structured workflows, guardrails, and checks enhances its performance and may even make new use cases a reality.</p> <p><strong><em>But aren‚Äôt we being too harsh on the LLMs?</em></strong> Yes, I believe we are!! Humans often rely on predefined workflows in their work environments, commonly called Standard Operating Procedures (SOPs) or simply ‚Äúhow-to‚Äù guides. Additionally, humans sometimes struggle with open-ended tasks where they must devise steps and adapt along the way.</p> <h1 id="4-conclusion-agi-when">4. Conclusion: AGI when??</h1> <p>Things are moving fast; you might find yourself outdated just by spending time reading this blog post instead of <em>vibe coding</em> with Gemini 2.5 Pro ~<em>GOD MODE</em>~. Jokes aside, the TLDR is: <strong>Don‚Äôt overly rely on LLM capabilities; give them structure, high-level tools, and output checks</strong>. Doing this will make you happier, and your clients will be happier.</p> <p>Full autonomy is the dream, but what will we build when it finally arrives? Will AI Engineers have any moat? Won‚Äôt the AGI providers + thin wrappers suffice for clients?</p> <p>Will AGI solve everything? I‚Äôm not sure‚Ä¶ We often provide a lot of context to newcomers in our companies. Will AGI be able to ingest this context like a human? Some may only label it AGI when it can, but is the average human significantly better than today‚Äôs leading models? (excluding spatial/physical reasoning). Do you even believe that the average human possesses good common sense?</p> <h1 id="appendix-a-why-do-ai-multi-agent-frameworks-like-crewai-and-autogen-work">Appendix A: Why do AI multi-agent frameworks like CrewAI and AutoGen work?</h1> <p>This section may be too small for a standalone blog post, but it fits nicely here as we talk about autonomy and workflows. We consistently observe that multi-agent workflows with LLMs outperform single conversations with a few back-and-forth exchanges, and‚Ä¶ this is expected!</p> <p>One thing to keep in mind about pre-trained-focused LLMs (e.g., GPT-4o, Claude 3, and Llama, not o1, R1, Claude 3.7-thinking) is that they have consumed all the data from the internet and are not just knowledge artifacts but also cultural ones! They encapsulate human behavior across various scenarios and contexts.</p> <p>So, when multi-agent frameworks like <a href="https://github.com/crewAIInc/crewAI">CrewAI</a> and <a href="https://github.com/ag2ai/ag2">AutoGen</a> are employed, they usually enforce:</p> <ul> <li><strong>Role playing</strong>: Assigning each agent a specific role and background, e.g., <em>‚ÄúYou are a marketing specialist with over 10 years of experience.‚Äù</em></li> <li><strong>Reflection</strong>: This typically occurs because one agent acts as the ‚Äúmanager‚Äù or ‚Äúreviewer‚Äù of another agent‚Äôs output.</li> </ul> <p>These two points represent well-known best practices when using LLMs. But why is this the case? Because they simulate human processes that are known to yield better results, such as reviewing each other‚Äôs <a href="https://chatgpt.com/share/67e4abc9-2068-800a-8f07-23d7264a0298">code PRs</a> and gathering multi-perspective opinions on subjects (similar to ‚Äúgroup chat rooms‚Äù that these frameworks provide). However, one valuable lesson learned over time is that <strong>while multi-agent systems with a lot of autonomy perform better than a single LLM using a <a href="https://www.promptingguide.ai/techniques/react">ReAct workflow</a>, they do not outperform an LLM workflow designed by domain experts.</strong></p> <div style="text-align: center;"> <img src="/assets/img/posts/2025-03-ineffectiveness-of-ai-agents/react-maw-wwhf.png" alt="AI Agents workflow designs" style="max-width: 70%; height: auto;"/> </div> <p style="text-align: center; margin-top: 0.5em;">Figure 7: ReAct vs. Multi-agent systems with full autonomy vs. Domain-expert-based workflow.</p>]]></content><author><name></name></author><category term="AI Agents"/><category term="LLMs"/><category term="GenAI"/><summary type="html"><![CDATA[Wait, where's my UBS deposit??]]></summary></entry><entry><title type="html">Escaping Dependency Hell: An Introduction to Devcontainers</title><link href="https://romulodrumond.com/blog/2023/escaping-dependency-hell/" rel="alternate" type="text/html" title="Escaping Dependency Hell: An Introduction to Devcontainers"/><published>2023-12-11T04:00:00+00:00</published><updated>2023-12-11T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2023/escaping-dependency-hell</id><content type="html" xml:base="https://romulodrumond.com/blog/2023/escaping-dependency-hell/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a></li> <li><a href="#2-what-are-devcontainers">2. What are Devcontainers?</a></li> <li><a href="#3-why-devcontainers">3. Why Devcontainers?</a></li> <li><a href="#4-how-do-devcontainers-compare">4. How do Devcontainers Compare?</a></li> <li><a href="#5-advantages-and-disadvantages-of-devcontainers">5. Advantages and Disadvantages of Devcontainers</a> <ul> <li><a href="#51-advantages">5.1. Advantages</a></li> <li><a href="#52-disadvantages">5.2. Disadvantages</a></li> </ul> </li> <li><a href="#6-devcontainer-in-a-web-scraping-project">6. Devcontainer in a web scraping project</a> <ul> <li><a href="#61-the-devcontainerjson-file">6.1. The <code class="language-plaintext highlighter-rouge">devcontainer.json</code> file</a></li> <li><a href="#62-the-dockerfile">6.2. The Dockerfile</a></li> <li><a href="#63-comparison-with-a-non-devcontainer-setup">6.3. Comparison with a non-devcontainer setup</a></li> </ul> </li> <li><a href="#7-using-devcontainers-in-vs-code">7. Using Devcontainers in VS Code</a> <ul> <li><a href="#71-essential-commands-for-devcontainers">7.1. Essential Commands for Devcontainers</a> <ul> <li><a href="#711-troubleshooting-common-errors">7.1.1. Troubleshooting Common Errors</a></li> </ul> </li> </ul> </li> <li><a href="#8-conclusion">8. Conclusion</a></li> <li><a href="#appendix-a-devcontainers-and-wsl">Appendix A: Devcontainers and WSL</a></li> <li><a href="#apendix-b-how-do-devcontainers-really-compare">Apendix B: How do Devcontainers REALLY Compare?</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>Development containers, or <strong>devcontainers</strong>, are standardized, isolated environments that developers can use to build, test, and deploy software. As an <a href="https://containers.dev/">open standard</a>, they support use across different Integrated Development Environments (IDEs), such as <a href="https://code.visualstudio.com/">Visual Studio Code</a> and <a href="https://www.jetbrains.com/pycharm/">PyCharm</a>.</p> <h1 id="2-what-are-devcontainers">2. What are Devcontainers?</h1> <p>Devcontainers is a tool powered by Docker that offers reliable development environments for projects. It‚Äôs worth noting that while Docker is the primary container runtime used, other runtimes such as Podman can also be utilized, though it may not be as straightforward. Devcontainers ensure that all dependencies and settings needed for a project are encapsulated and can be easily replicated. With the <code class="language-plaintext highlighter-rouge">devcontainer.json</code> configuration file, you can define the environment down to the detail, setting up a specific version of a programming language, the necessary libraries, or even particular code editor settings.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-12-13-escaping-dependency-hell/architecture-containers-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-12-13-escaping-dependency-hell/architecture-containers-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-12-13-escaping-dependency-hell/architecture-containers-1400.webp"/> <img src="/assets/img/posts/2023-12-13-escaping-dependency-hell/architecture-containers.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1 - Architecture of devcontainers in VSCode.<br/>Source: https://code.visualstudio.com/docs/devcontainers/containers</figcaption> </figure> </div> <style>@media(min-width:768px){.img-fluid{width:80%}}@media(max-width:767px){.img-fluid{width:100%}}</style> <h1 id="3-why-devcontainers">3. Why Devcontainers?</h1> <p>One of the significant challenges in software development and data science is maintaining consistency in development environments. A funny common chorus you might hear is: ‚ÄúBut it works on my machine!‚Äù This inconsistency becomes even more of a headache when you‚Äôre switching between projects that require different versions of the same language, or different package managers.</p> <p>Imagine one project using Python 3.7 with pip, while another one uses Python 3.9 with <a href="https://python-poetry.org/">Poetry</a>. Switching between these projects without some form of isolation could quickly lead to version conflicts and a lot of frustration.</p> <p>That‚Äôs where devcontainers are useful. They isolate each project‚Äôs environment into its container, solving the inconsistencies and making it easier to switch between projects (and rebuild a broken environment from scratch!). All the while, you maintain your familiar IDE setup.</p> <h1 id="4-how-do-devcontainers-compare">4. How do Devcontainers Compare?</h1> <p>Other solutions to the problem of environment consistency exist, such as virtual machines or manual dependency installations. But these methods come with their challenges - virtual machines are resource-intensive and slow, while manual setups require careful documentation and sometimes a lot of time to configure correctly (notice that this problem may arise only after setting up multiple projects in the same machine).</p> <p>Devcontainers, in contrast, offer a more lightweight and efficient solution. They start up quickly, require fewer system resources, and above all, they are easily reproducible, thanks to Docker and the open standard defined by <code class="language-plaintext highlighter-rouge">devcontainer.json</code>.</p> <h1 id="5-advantages-and-disadvantages-of-devcontainers">5. Advantages and Disadvantages of Devcontainers</h1> <h2 id="51-advantages">5.1. Advantages</h2> <ul> <li><strong>Consistency</strong>: Devcontainers ensure that every developer is working in an identical environment, eliminating the classic ‚Äúit works on my machine‚Äù problem;</li> <li><strong>Isolation</strong>: Each project‚Äôs dependencies are kept separate, which minimizes the risk of conflicts between different projects that might require different versions of the same package or software;</li> <li><strong>Reproducibility</strong>: The entire environment can be easily replicated and shared among team members, making the onboarding process for new developers much smoother. Also, the environment can be easily deleted and rebuilt again;</li> <li><strong>Flexibility</strong>: Devcontainers adhere to an open standard, meaning they work across different IDEs, including Visual Studio Code and PyCharm;</li> <li><strong>Automation</strong>: With the use of a <code class="language-plaintext highlighter-rouge">devcontainer.json</code> file, the setup of a development environment can be automated, which saves time for developers.</li> </ul> <h2 id="52-disadvantages">5.2. Disadvantages</h2> <ul> <li><strong>Dependency on Docker</strong>: Since devcontainers rely on Docker, developers need to have Docker installed and running on their machine. This might pose challenges in certain operating systems or in environments where Docker is not preferred;</li> <li><strong>Learning Curve</strong>: Though not steep, there is still a learning curve associated with understanding Docker and the concept of devcontainers. This can pose an initial hurdle, particularly for developers who are not familiar with containerization.</li> </ul> <p>In the following sections, we‚Äôll explore how to use devcontainers in a practical example and provide some tips based on personal experience.</p> <h1 id="6-devcontainer-in-a-web-scraping-project">6. Devcontainer in a web scraping project</h1> <p>In the upcoming section, a practical example will be provided, demonstrating how to set up a devcontainer in a web scraping project. The configuration process primarily involves two key files: <code class="language-plaintext highlighter-rouge">devcontainer.json</code> and <code class="language-plaintext highlighter-rouge">Dockerfile</code>. However, in simpler projects, it might be feasible to configure the devcontainer using only the <code class="language-plaintext highlighter-rouge">devcontainer.json</code> file.</p> <h2 id="61-the-devcontainerjson-file">6.1. The <code class="language-plaintext highlighter-rouge">devcontainer.json</code> file</h2> <p>The devcontainer.json file sets the foundation for our containerized development environment. Here‚Äôs what it looks like:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"scrapest"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"build"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"dockerfile"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Dockerfile"</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"postCreateCommand"</span><span class="p">:</span><span class="w"> </span><span class="s2">"make install"</span><span class="p">,</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>The ‚Äúbuild‚Äù field points to the Dockerfile that defines the container. Upon creation of the container, the <code class="language-plaintext highlighter-rouge">make install</code> command is run, which installs our project‚Äôs dependencies based on a command defined on a <code class="language-plaintext highlighter-rouge">Makefile</code>, in a simple Python project it could be simply a <code class="language-plaintext highlighter-rouge">poetry install</code>.</p> <h2 id="62-the-dockerfile">6.2. The Dockerfile</h2> <p>The <code class="language-plaintext highlighter-rouge">Dockerfile</code> outlines how to build the Docker container:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> mcr.microsoft.com/devcontainers/python:1-3.9-buster</span>

<span class="k">RUN </span><span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    wget &lt;https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz&gt; <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">tar</span> <span class="nt">-xvzf</span> geckodriver<span class="k">*</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">chmod</span> +x geckodriver <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">mv </span>geckodriver /usr/local/bin/ <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    firefox-esr  <span class="se">\
</span>    pip <span class="nb">install </span><span class="nv">poetry</span><span class="o">==</span>1.5.0
</code></pre></div></div> <p>This container starts from a base image preloaded with Python 3.9. We then install <code class="language-plaintext highlighter-rouge">geckodriver</code> and <code class="language-plaintext highlighter-rouge">firefox-esr</code> to perform browser tasks.</p> <p>Most importantly, we install Poetry using pip, <strong>explicitly specifying version 1.5.0</strong> to ensure consistent behavior across all environments. Given that different projects may require different versions of Poetry and Python (Poetry itself doesn‚Äôt handle Python versions the same way <a href="https://docs.conda.io/projects/conda/en/stable/">Conda</a> does), isolating the Poetry and Python versions within each project‚Äôs devcontainer helps avoid compatibility issues and headaches with setup.</p> <h2 id="63-comparison-with-a-non-devcontainer-setup">6.3. Comparison with a non-devcontainer setup</h2> <p>Without devcontainers, each project‚Äôs environment setup can become a complex task. You would need to manage different Python, Poetry, and web driver versions manually, resolve potential dependency conflicts between projects, and spend valuable time ensuring the machine is correctly set up. By adopting devcontainers, we isolate each project in its environment with the correct dependency versions, providing a consistent, reproducible setup.</p> <h1 id="7-using-devcontainers-in-vs-code">7. Using Devcontainers in VS Code</h1> <p>When utilizing devcontainers within Visual Studio Code, familiarity with a few key commands and best practices can enhance your workflow. To access these commands, open the Command Palette by pressing <code class="language-plaintext highlighter-rouge">Ctrl + Shift + P</code>. All the commands listed in this section start with the prefix ‚Äú<code class="language-plaintext highlighter-rouge">Dev Containers:</code>‚Äù. This section will elaborate on these essentials based on my own experiences.</p> <h2 id="71-essential-commands-for-devcontainers">7.1. Essential Commands for Devcontainers</h2> <p>Some commands are important when working with devcontainers:</p> <ul> <li><code class="language-plaintext highlighter-rouge">Reopen in Container</code>: Once your devcontainer configuration is in place, you can start your development environment using this command. This command interprets your <code class="language-plaintext highlighter-rouge">devcontainer.json</code> file and builds the Docker container accordingly. If the container was already built previously, it just opens the IDE connected to the container;</li> <li><code class="language-plaintext highlighter-rouge">Reopen Folder Locally</code>: If you need to switch back to your local environment, this command allows you to quickly transition out of the devcontainer setup.</li> <li><code class="language-plaintext highlighter-rouge">Reopen Folder on WSL</code>: For those using Windows Subsystem for Linux (WSL), this command comes in handy when you want to reopen your project within WSL.</li> <li><code class="language-plaintext highlighter-rouge">Rebuild Container</code>: When changes are made to your <code class="language-plaintext highlighter-rouge">devcontainer.json</code> or Dockerfile, use this command to ensure your new settings are applied.</li> <li><code class="language-plaintext highlighter-rouge">Rebuild Without Cache and Reopen in Container</code>: If you want to ensure a clean rebuild of your container, bypassing Docker‚Äôs layer caching, use this command. This is especially useful when you want to avoid any artifacts from previous builds.</li> </ul> <h3 id="711-troubleshooting-common-errors">7.1.1. Troubleshooting Common Errors</h3> <p>As you work with devcontainers, you might encounter certain issues. Here are a couple of the most common errors and their potential solutions:</p> <ul> <li><strong>Container build failures</strong>: These can arise from errors in your <code class="language-plaintext highlighter-rouge">Dockerfile</code> or <code class="language-plaintext highlighter-rouge">devcontainer.json</code>. Check these files for any potential errors.</li> <li><strong>Connectivity problems</strong>: If you‚Äôre having trouble connecting to the container, ensure that Docker is running correctly. Often, a simple restart of Docker can resolve these issues.</li> </ul> <h1 id="8-conclusion">8. Conclusion</h1> <p>Devcontainers represent a key shift in software development and data science, addressing the perennial challenge of environment inconsistency, also known as the ‚Äúit works on my machine‚Äù problem. By leveraging Docker‚Äôs containerization, devcontainers ensure uniformity, isolation, and reproducibility in development environments, simplifying project setups.</p> <p>While they come with a learning curve and a dependency on Docker, the advantages they offer in streamlining development workflows are significant. They not only facilitate a hassle-free setup for diverse projects but also ensure that team members can easily collaborate with identical setups.</p> <p>As the adoption of devcontainers grows, their role in modern development practices is becoming increasingly essential. They aren‚Äôt just a tool to solve current challenges but a strategic asset for future-proofing development environments.</p> <h1 id="appendix-a-devcontainers-and-wsl">Appendix A: Devcontainers and WSL</h1> <p>If you‚Äôre running on Windows Subsystem for Linux (WSL) and require audio output for your applications, you may add the following lines under ‚Äúmounts‚Äù and ‚ÄúcontainerEnv‚Äù. This will direct the container to use your system‚Äôs Pulse Server for sound. I use this often because I rely on audio output to know when a long process has finished (<code class="language-plaintext highlighter-rouge">speech-dispatcher</code> commands inside the container will work properly).</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"scrapest"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"build"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"dockerfile"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Dockerfile"</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"postCreateCommand"</span><span class="p">:</span><span class="w"> </span><span class="s2">"make install"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"mounts"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
		</span><span class="s2">"source=/mnt/wslg,target=/mnt/wslg,type=bind"</span><span class="w">
	</span><span class="p">],</span><span class="w">
	</span><span class="nl">"containerEnv"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"PULSE_SERVER"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${localEnv:PULSE_SERVER}"</span><span class="w">
	</span><span class="p">},</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>Another pain point you may have is with your SSH keys that are on WSL and not on the container. For that you will need an SSH agent when building the container, you can do so by adding the following lines to your <code class="language-plaintext highlighter-rouge">.zshrc</code>/<code class="language-plaintext highlighter-rouge">.bashrc</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$SSH_AUTH_SOCK</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
   <span class="c"># Check for a currently running instance of the agent</span>
   <span class="nv">RUNNING_AGENT</span><span class="o">=</span><span class="s2">"</span><span class="sb">`</span>ps <span class="nt">-ax</span> | <span class="nb">grep</span> <span class="s1">'ssh-agent -s'</span> | <span class="nb">grep</span> <span class="nt">-v</span> <span class="nb">grep</span> | <span class="nb">wc</span> <span class="nt">-l</span> | <span class="nb">tr</span> <span class="nt">-d</span> <span class="s1">'[:space:]'</span><span class="sb">`</span><span class="s2">"</span>
   <span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$RUNNING_AGENT</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"0"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
        <span class="c"># Launch a new instance of the agent</span>
        ssh-agent <span class="nt">-s</span> &amp;&gt; <span class="nv">$HOME</span>/.ssh/ssh-agent
   <span class="k">fi</span>
   <span class="o">{</span> <span class="nb">eval</span> <span class="sb">`</span><span class="nb">cat</span> <span class="nv">$HOME</span>/.ssh/ssh-agent<span class="sb">`</span><span class="p">;</span> ssh-add ~/.ssh/id_ed25519 <span class="o">&gt;&gt;</span> /dev/null <span class="o">}</span> &amp;&gt; /dev/null
<span class="k">fi</span>
</code></pre></div></div> <h1 id="apendix-b-how-do-devcontainers-really-compare">Apendix B: How do Devcontainers REALLY Compare?</h1> <p>Addressing the challenge of environment consistency in development, various tools and methods have been employed, each with its advantages and limitations. Let‚Äôs consider the traditional approaches like virtual machines and manual dependency installations, alongside package managers like pip, pipx, Conda, and Poetry, before delving into how devcontainers offer a comprehensive solution.</p> <p>Virtual machines (VMs) are a common approach for environment isolation. They provide complete OS-level separation but at a significant cost in terms of resource intensity and speed. VMs often consume considerable system resources and can be slow to start and operate, making them less efficient for rapid development cycles.</p> <p>Manual dependency installations, on the other hand, offer more control but come with the challenge of meticulous documentation and time-consuming setup processes. This approach becomes increasingly complex with multiple projects, each requiring different versions of dependencies, leading to a high maintenance overhead (i.e. dependency hell).</p> <p>In the realm of Python development, tools like pip, pipx, Conda, and Poetry have been widely adopted for managing packages and dependencies. Pip is the standard package manager, but it does not inherently solve the problem of environment isolation due to the need to have multiple Python versions. Pipx steps in to provide a layer of isolation for Python-based command-line tools. Conda extends these capabilities further, offering package management across multiple languages and environments (you can isolate Node.js and (Cuda)[https://blogs.nvidia.com/blog/what-is-cuda-2/] versions on a Conda environment). However, Conda‚Äôs approach can sometimes lead to bulky environments due to its extensive package repository and lack of reproducibility due to not natively supporting .lock files.</p> <p>Poetry, a relatively newer entrant, simplifies dependency management and packaging in Python. It combines package management and environment management, aiming to provide a unified tool for Python projects. Despite its benefits, Poetry alone cannot eliminate the issues arising from system-wide package installations and environment configurations, especially when different projects require different Python versions or dependencies (you could use <a href="https://github.com/pyenv/pyenv">pyenv</a> for different Python versions, but honestly, devcontainers make the job much easier).</p> <p>This is where devcontainers come into the picture, addressing the gaps left by these tools and methods. Unlike virtual machines, devcontainers are just containers: lightweight and quick to start, requiring far fewer system resources. They provide an isolated, Docker-powered environment for each project, eliminating the complexities associated with manual setups. You could have different web scraping/automation software using different versions of the <code class="language-plaintext highlighter-rouge">geckodriver</code>, with no interference between them. The <code class="language-plaintext highlighter-rouge">devcontainer.json</code> file, adhering to an open standard, allows for easy replication and sharing of development environments across teams (you could also push the container image so the team uses exactly the same environment in development).</p> <p>Devcontainers essentially encapsulate the entire development setup ‚Äî including specific versions of languages, libraries, and even IDE settings ‚Äî in a containerized environment. This ensures that all developers work in an identical environment, regardless of their local machine setup. This level of consistency and isolation is hard to achieve with traditional tools like pip, pipx, Conda, or even Poetry.</p> <p>In summary, while tools like pip, pipx, Conda, and Poetry offer solutions to parts of the environment consistency problem, devcontainers provide a comprehensive answer. They bring together the benefits of isolation, lightweight operation, and reproducibility in a way that other methods and tools cannot, making them an invaluable asset in modern software development workflows.</p>]]></content><author><name></name></author><category term="WSL"/><category term="MLOps"/><category term="Devcontainers"/><summary type="html"><![CDATA[1. Introduction 2. What are Devcontainers? 3. Why Devcontainers? 4. How do Devcontainers Compare? 5. Advantages and Disadvantages of Devcontainers 5.1. Advantages 5.2. Disadvantages 6. Devcontainer in a web scraping project 6.1. The devcontainer.json file 6.2. The Dockerfile 6.3. Comparison with a non-devcontainer setup 7. Using Devcontainers in VS Code 7.1. Essential Commands for Devcontainers 7.1.1. Troubleshooting Common Errors 8. Conclusion Appendix A: Devcontainers and WSL Apendix B: How do Devcontainers REALLY Compare?]]></summary></entry><entry><title type="html">Navigating the Web with Python: Insights into Scraping and Automation Tools</title><link href="https://romulodrumond.com/blog/2023/navigating-the-web-with-python/" rel="alternate" type="text/html" title="Navigating the Web with Python: Insights into Scraping and Automation Tools"/><published>2023-12-06T04:00:00+00:00</published><updated>2023-12-06T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2023/navigating-the-web-with-python</id><content type="html" xml:base="https://romulodrumond.com/blog/2023/navigating-the-web-with-python/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a></li> <li><a href="#2-tool-overview">2. Tool overview</a> <ul> <li><a href="#21-beautiful-soup">2.1. Beautiful Soup</a></li> <li><a href="#22-selenium">2.2. Selenium</a></li> <li><a href="#23-scrapy">2.3. Scrapy</a></li> </ul> </li> <li><a href="#3-choosing-the-right-tool-for-the-job">3. Choosing the right tool for the job</a></li> <li><a href="#4-tldr">4. TLDR</a></li> <li><a href="#appendix-a-exploring-additional-webpage-rendering-tools">Appendix A: Exploring Additional Webpage Rendering Tools</a></li> <li><a href="#appendix-b-waiting-for-browser-rendering-sucks">Appendix B: Waiting for browser rendering sucks</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>No developer can afford not to know or interact with web technologies. From APIs to network security protocols that one needs to attend for the company, a developer, and even ‚Äòdata‚Äô people, end up interacting with HTTP requests, VPNs, and network boundaries. But fancy jargon aside this post is not going to be either about serving on the web, developing a web application on Django or Flask, or exposing an API with FastAPI, we gonna talk about being a client, a consumer, being ‚Äòserved by the web‚Äô.</p> <p>When do we act as a web client outside of traditional web browsing? This often occurs in the realms of <strong>web scraping</strong> and <strong>web automation</strong>. Web scraping involves programmatically extracting data from websites, while web automation refers to automating web-based tasks, sometimes known as <a href="https://en.wikipedia.org/wiki/Robotic_process_automation">Robotic Process Automation (RPA)</a>. These practices are especially relevant when dealing with legacy systems or in scenarios where direct user interaction is restricted. In this context, I want to discuss three widely used tools intended for these applications: <strong>Beautiful Soup</strong>, <strong>Selenium</strong>, and <strong>Scrapy</strong>. Each of these tools offers unique features and capabilities, making them go-to choices for web scraping and automation tasks.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy-1400.webp"/> <img src="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_vs_bs4_vs_scrapy.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1 - The Web Warriors Civil War.</figcaption> </figure> </div> <style>@media(min-width:768px){.img-fluid{width:50%}}@media(max-width:767px){.img-fluid{width:100%}}</style> <h1 id="2-tool-overview">2. Tool overview</h1> <h2 id="21-beautiful-soup">2.1. Beautiful Soup</h2> <p>Is the simplest of the three, it is basically an HTML/XML parser that provides a more user-friendly interface for iterating, searching, and parsing the HTML document tree.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer-1400.webp"/> <img src="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/beautifulsoup_serving_a_developer.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2 - Beautiful Soup serving a delicious Python object to a developer.</figcaption> </figure> </div> <p>For fetching/acquiring the data to be parsed by Beautiful Soup you will need other tools such as Python‚Äôs <code class="language-plaintext highlighter-rouge">requests</code> built-in library to make the HTTP requests to the web services. Given its parsing nature, Beautiful Soup is a library used more for web scraping when someone wants to abstract a bit the difficulties of understanding <a href="https://www.w3schools.com/xml/xpath_syntax.asp"><em>xpaths</em></a> and <a href="https://www.w3schools.com/cssref/css_selectors.php"><em>css selectors</em></a>. A common example of using this library can be found below.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="c1"># URL of the webpage to scrape
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://books.toscrape.com/</span><span class="sh">'</span>

<span class="c1"># Send HTTP request to the URL
</span><span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># Check if the request was successful
</span><span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="c1"># Parse the content of the response
</span>    <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">html.parser</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Find all h3 tags (which contain book titles)
</span>    <span class="n">book_titles</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Extract the titles and write to a file
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">book_titles.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">book_titles</span><span class="p">):</span>
            <span class="n">book_title</span> <span class="o">=</span> <span class="n">title</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">]</span>
            <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">book_title</span><span class="si">}</span><span class="sh">"</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
            <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">line</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Failed to retrieve the webpage. Status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1: A Light in the Attic
2: Tipping the Velvet
3: Soumission
4: Sharp Objects
5: Sapiens: A Brief History of Humankind
...
</code></pre></div></div> <h2 id="22-selenium">2.2. Selenium</h2> <p>Selenium is kind of a big monster that has made a name for itself among web developers, primarily because it was originally developed for automated software testing of web applications. Its versatility is further showcased by its support for multiple programming languages, including Java, Python, C#, Ruby, JavaScript, and Kotlin. Unlike libraries that are limited to parsing HTML, Selenium offers users the ability to control a full-fledged browser, automating a wide range of tasks across <a href="https://www.selenium.dev/documentation/webdriver/browsers/">several supported browsers</a>.</p> <div style="text-align: center"> <figure> <picture> <img src="/assets/img/posts/2023-11-30-from-web-scraping-to-automation-pythons-tools-overview/selenium_automation.gif" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Gif 1 - Example of Selenium controlling a Firefox browser.</figcaption> </figure> </div> <p>As you may have already noticed, Selenium is pretty friendly with the idea of automating web tasks, i.e. RPA, as it can click on buttons, and scroll the page, just like simulating a human interaction with the browser. But Selenium is also used for web scraping sometimes. <em>Why is that?</em> You may ask. Because scraping JavaScript-heavy (JS-heavy) websites is not as straightforward as doing HTTP requests. For example:</p> <ol> <li>Browse to the <a href="https://www.openweb.com">www.openweb.com</a> webpage</li> <li>Open the developer tools (F12 usually);</li> <li>Disable the JavaScript. You can look for the check box or press ctrl + shift + P, then ‚Äúdisable javascript‚Äù on the search box;</li> <li>Update the page (F5);</li> </ol> <p>You will notice that things don‚Äôt look the same. You may also make an HTTP request to this page and check the returned HTML doesn‚Äôt have all the information you can see on the HTML of your browser.</p> <p>Now you see that things are not what they look like. Web pages on JS-heavy sites may appear static, but there‚Äôs more than meets the eye. Often, these pages dynamically load content, initiating new requests for assets after the initial page render. This is due to the site‚Äôs JavaScript, which fetches additional data and updates the display in real time. For those new to web development, a practical way to observe this is by opening the ‚Äònetwork‚Äô tab in your browser‚Äôs developer tools. There, you can monitor the various requests your browser makes to fully render the page.</p> <p>Because of these types of sites Selenium shines in the scrappy activity, as it is a full browser, it automatically runs all the JS that is needed to render the page for you, landing the data you want to acquire directly on the modified page HTML.</p> <p><em>Wait! For those JS-heavy sites, I can only hope to wait for a tool like Selenium to render the page?</em></p> <p>Actually no, you can replicate the requests you found on the ‚Äònetwork‚Äô tab on your Python code, using Selenium just makes the job pretty easy, despite adding a lot of latency and asking for more computing resources.</p> <h2 id="23-scrapy">2.3. Scrapy</h2> <p>Scrapy is a bit different from Beautiful Soup and Selenium. Scrapy is a framework, not a library. Instead of writing code that uses Scrapy, you write code that the Scrapy framework will use. For example, if you want to craw and dump a whole website you just need to:</p> <ol> <li>Run the scrappy create command: <code class="language-plaintext highlighter-rouge">scrapy startproject &lt;project-name&gt;</code>;</li> <li>Put a code similar to this one in the spider:</li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">scrapy</span>
<span class="kn">from</span> <span class="n">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="n">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">ExampleCrawlSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">examplecrawlspider</span><span class="sh">'</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">example.com</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Replace with the target domain
</span>    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">http://example.com</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Replace with the starting URL
</span>
    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c1"># Rule to follow links (LinkExtractor), calling the defined 
</span>        <span class="c1"># 'parse_item' method
</span>        <span class="nc">Rule</span><span class="p">(</span><span class="nc">LinkExtractor</span><span class="p">(),</span> <span class="n">callback</span><span class="o">=</span><span class="sh">'</span><span class="s">parse_item</span><span class="sh">'</span><span class="p">,</span> <span class="n">follow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># Extract the entire content of the &lt;body&gt; tag from the response
</span>        <span class="n">page_content</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">css</span><span class="p">(</span><span class="sh">'</span><span class="s">body</span><span class="sh">'</span><span class="p">).</span><span class="nf">get</span><span class="p">()</span>

        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">dumped_pages/</span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">.html</span><span class="sh">'</span>
        <span class="k">if</span> <span class="n">filename</span> <span class="o">==</span> <span class="sh">'</span><span class="s">dumped_pages/.html</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="sh">'</span><span class="s">dumped_pages/index.html</span><span class="sh">'</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="sh">'</span><span class="s">dumped_pages</span><span class="sh">'</span><span class="p">):</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">dumped_pages</span><span class="sh">'</span><span class="p">)</span>

        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">page_content</span><span class="p">.</span><span class="nf">encode</span><span class="p">())</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Saved file </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Note: You might need to change some configurations before running the spider</p> <p>As you may imagine, by its name, Scrappy is the go-to framework for large-scale scraping and crawling. Did you start scraping with Python and now want to put things async with aiohttp? Scrapy is async by default. Need to add data validation to the extracted information? Need to do automatic throttling? Banned detection? Add/swap middleware? Distributed multi-node crawling? <strong>Scrapy can do it all</strong>.</p> <p>Don‚Äôt get me wrong, I know that using a framework like Scrapy adds complexity and new things to learn, but if you think you can go a long way with Python‚Äôs request + Beautiful Soup is because you can‚Äôt see how far away you would if using Scrapy.</p> <p>Note: <a href="https://www.youtube.com/@JohnWatsonRooney">John Watson Rooney</a> is a pretty good channel with tutorials about Scrapy and other web scraping tools.</p> <h1 id="3-choosing-the-right-tool-for-the-job">3. Choosing the right tool for the job</h1> <p>Deciding on the right tool can be overwhelming. To simplify the process, consider two main factors: the scope of your project ‚Äî including the number of sites, requests, operations, or items involved ‚Äî and your primary goal, which might range from web testing to simple or extensive web scraping. Wondering how to choose between the three? Below is a table that summarizes their key attributes and uses:</p> <table> <thead> <tr> <th><strong>Feature/Tool</strong></th> <th><strong>Beautiful Soup</strong></th> <th><strong>Selenium</strong></th> <th><strong>Scrapy</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Primary Use</strong></td> <td>HTML/XML Parsing</td> <td>Web Automation</td> <td>Large-Scale Web Scraping</td> </tr> <tr> <td><strong>Ease of Use</strong></td> <td>Easy for beginners</td> <td>Moderate</td> <td>Steeper learning curve</td> </tr> <tr> <td><strong>Performance</strong></td> <td>Fast for simple tasks</td> <td>Slower, resource-intensive</td> <td>Fast, efficient for large tasks</td> </tr> <tr> <td><strong>JS Heavy Sites</strong></td> <td>Limited (needs extra handling)</td> <td>Excellent (renders JS)</td> <td>Good (with some workarounds)</td> </tr> <tr> <td><strong>Scalability</strong></td> <td>Ideal for small projects</td> <td>Not ideal for large-scale scraping</td> <td>Highly scalable</td> </tr> <tr> <td><strong>Integration</strong></td> <td>Works with Python requests</td> <td>Full-fledged browser control</td> <td>Extensive framework capabilities</td> </tr> <tr> <td><strong>Best For</strong></td> <td>Quick scraping tasks, learning basics</td> <td>Automating browser tasks, handling JS-heavy sites</td> <td>Advanced web scraping, large-scale projects</td> </tr> </tbody> </table> <p><br/></p> <p>So, in the end, if you are looking for automation go to Selenium and if you are looking for web scraping go for Beautiful Soup or/and Scrapy (nothing prohibits you from using both).</p> <p><em>Wait, and what about the JS-heavy sites?</em></p> <p>Yeah, Selenium makes things easy on this front, but it‚Äôs really slow and hardware-hungry (it‚Äôs a browser in the end, everybody knows that they are becoming almost as containerized operational systems (OS)). If you need to cut costs and be blazing fast you need to go raw and do the extra request by yourself (inspect the website with the dev tools and discover what request gets the data you want) and replicate it in Beautiful Soup/Scrapy. <strong>That‚Äôs a no-brainer: when scraping, rendering is helpful for the human eye, but remember that you don‚Äôt want to render things, what you really want is the data.</strong></p> <h1 id="4-tldr">4. TLDR</h1> <p><em>What to choose if I‚Äôm starting and don‚Äôt need to do a lot of requests?</em></p> <ul> <li>Beautiful Soup.</li> <li><em>But what if the site is JS-heavy?</em> <ul> <li>Selenium or Playwright.</li> </ul> </li> </ul> <p><em>What if I do need a lot of requests?</em> (being I a beginner or not)</p> <ul> <li>Go for Scrapy (if you are a beginner, be brave, the effort will pay off).</li> </ul> <p><em>If I want to automate web interactions, maybe RPA?</em></p> <ul> <li>Selenium/Playwright.</li> <li>You might go raw with BeatitulSoup/Scrapy, translating click and API calls, it depends on your project‚Äôs requirements. Going raw will ask you to understand the web better (API calls, headers, sessions, cookies).</li> </ul> <h1 id="appendix-a-exploring-additional-webpage-rendering-tools">Appendix A: Exploring Additional Webpage Rendering Tools</h1> <p>While I‚Äôve highlighted the use of raw HTTP requests and Selenium, it‚Äôs worth noting that the ecosystem of tools for webpage rendering is rich with options. You can explore tools like <a href="https://playwright.dev/python/docs/api/class-playwright">Playwright</a>, which offers browser automation capabilities such as Selenium, or <a href="https://github.com/scrapy-plugins/scrapy-splash">Splash</a>, a lightweight browser render service that integrates well with Scrapy. For those seeking a more managed solution, services like <a href="https://docs.zyte.com/zyte-api/usage/http.html#html-and-browser-html">Zyte</a> can render pages on your behalf.</p> <p>Transitioning to raw HTTP requests can be a smooth process. Here‚Äôs an approach to deepen your web scraping techniques progressively:</p> <ol> <li>Begin with Python‚Äôs <code class="language-plaintext highlighter-rouge">requests</code> library in pair with Beautiful Soup. This will introduce you to the basics of web scraping and HTML parsing.</li> <li>Move on to browser-based scraping with Selenium to handle JavaScript-heavy websites. You can continue to use Beautiful Soup to parse the HTML content fetched.</li> <li>Integrate Splash with Scrapy if you‚Äôre working within the Scrapy framework and need to render JavaScript without a full-fledged browser.</li> <li>Consider using Zyte‚Äôs rendering service for a server-side solution that can scale and handle complex scraping tasks without the need to manage browsers or proxies yourself.</li> <li>When ready, make the leap to Scrapy for an all-in-one framework experience, providing you with asynchronous processing capabilities and a wide range of built-in features for large-scale web scraping projects.</li> </ol> <p>By pacing your learning this way, you‚Äôll build a solid foundation in web scraping and automation, allowing you to choose the right tool for each job with more confidence.</p> <h1 id="appendix-b-waiting-for-browser-rendering-sucks">Appendix B: Waiting for browser rendering sucks</h1> <p>Browser rendering delays are a common pain point in web scraping, particularly on JavaScript-rich sites. Playwright claims to handle these delays more adeptly than Selenium, though I‚Äôve yet to thoroughly test this myself. From experience, Selenium‚Äôs built-in wait functions often fall short on such sites. I once had to engineer custom wait functions to deal with a particularly tricky site, which required me to measured max image dissimilarity between sequential screenshots to determine when the page had fully loaded.</p>]]></content><author><name></name></author><category term="Python"/><category term="Web scraping"/><category term="Web automation"/><summary type="html"><![CDATA[Because manually browsing the web is so 1990s.]]></summary></entry><entry><title type="html">Finishing my Data Career Masterplan</title><link href="https://romulodrumond.com/blog/2023/finishing-my-data-career-masterplan/" rel="alternate" type="text/html" title="Finishing my Data Career Masterplan"/><published>2023-10-06T04:00:00+00:00</published><updated>2023-10-06T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2023/finishing-my-data-career-masterplan</id><content type="html" xml:base="https://romulodrumond.com/blog/2023/finishing-my-data-career-masterplan/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a></li> <li><a href="#2-role-complexity-and-intersecting-responsibilities">2. Role Complexity and Intersecting Responsibilities</a> <ul> <li><a href="#21-why-role-overlap-exists">2.1. Why Role Overlap Exists</a></li> <li><a href="#22-core-responsibilities-in-each-role">2.2. Core Responsibilities in Each Role</a></li> <li><a href="#23-what-roles-can-learn-from-each-other">2.3. What Roles Can Learn From Each Other</a></li> </ul> </li> <li><a href="#3-engineering-vs-data-science-in-companies">3. Engineering vs. Data Science in Companies</a></li> <li><a href="#4-conclusion">4. Conclusion</a></li> <li><a href="#5-tldr">5. TLDR</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>In today‚Äôs data world, the lines between Data Engineering (DE), Data Science (DS), and Machine Learning Engineering (MLE) are blurry. This isn‚Äôt a mere assumption‚ÄîI‚Äôve walked the path through each of these roles myself (you can verify that on my <a href="https://www.linkedin.com/in/romulo-drumond/">LinkedIn</a>). Why, you ask? The simple answer: to explore the end-to-end pipeline of data products.</p> <p>The primary objective of this post is to share my experiences and insights into these roles, shed light on their overlapping responsibilities, especially in smaller organizations like startups, and finally help you understand what you‚Äôre signing up for when you see that job posting. Trust me, the job titles often provide little information about the job you will be doing.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_de_the_ds_and_the_mle-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_de_the_ds_and_the_mle-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_de_the_ds_and_the_mle-1400.webp"/> <img src="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_de_the_ds_and_the_mle.jpg" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1 - The DS, The DE and The MLE.</figcaption> </figure> </div> <style>@media(min-width:768px){.img-fluid{width:65%}}@media(max-width:767px){.img-fluid{width:100%}}</style> <h1 id="2-role-complexity-and-intersecting-responsibilities">2. Role Complexity and Intersecting Responsibilities</h1> <h2 id="21-why-role-overlap-exists">2.1. Why Role Overlap Exists</h2> <p>Role ambiguity in DE, DS, and MLE is particularly noticeable in small companies and startups. The primary driver of this problem? Resource constraints. When you‚Äôre operating on a shoestring budget and rapid timelines, multi-disciplinarity is not just an extra‚Äîit‚Äôs a necessity. But you might see this in big companies too, in those cases they are usually trying to get their feet wet on the data/ML world, so they try to minimize risk by contracting a jack-of-all-trades employee to run the first proofs of concept (POCs). Therefore, the emergence of the Full Stack Data Scientist (FSDS), an unicorn expected to have a holistic skill set, running the gamut from data acquisition to delivering machine learning (ML) models through customer-facing APIs.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_fsds_appears-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_fsds_appears-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_fsds_appears-1400.webp"/> <img src="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/the_fsds_appears.jpg" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2 - The trio is baffled by the appearance of a mythical creature called FSDS.</figcaption> </figure> </div> <p>The term FSDS is not just startup jargon; it encapsulates the reality of many job descriptions. The overlap exists because organizations are in a constant state of defining and redefining what each role should take ownership of. And let‚Äôs not forget, that the pace of technological advancements is merciless, adding another layer of complexity.</p> <h2 id="22-core-responsibilities-in-each-role">2.2. Core Responsibilities in Each Role</h2> <p>While the responsibilities may overlap, each role possesses a unique focus within the data pipeline. In a few words:</p> <ul> <li>Data Engineers: focus on getting and orchestrating the data for the organization, ensuring efficient storage solutions;</li> <li>Data Scientists: focus on analytics of the company‚Äôs data and on developing ML models to mine actionable insights;</li> <li>Machine Learning Engineers: focus on deploying and monitoring ML models and dealing with the peculiarities of these ‚Äúdata-fueled‚Äù software entities, known for their opaque operational mechanics.</li> </ul> <p>So, naturally, they fit in the data pipeline just like in Figure 3.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/roles_on_data_pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/roles_on_data_pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/roles_on_data_pipeline-1400.webp"/> <img src="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/roles_on_data_pipeline.png" class="img-fluid rounded z-depth-1" width="90%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 3 - Roles boundaries on a data pipeline.</figcaption> </figure> </div> <p>As you may see, we naturally have some overlapping in the data products pipeline (DE requires to know DS data needs, DS may need to know some serving constraints such as latency for deciding model size, etc.) but the day-to-day activities and skills overlap differently, just like Figure 4.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/veen_diagram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/veen_diagram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/veen_diagram-1400.webp"/> <img src="/assets/img/posts/2023-10-06-finishing-my-data-career-masterplan/veen_diagram.jpeg" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 4 - Skill intersections of the roles.</figcaption> </figure> </div> <p>This diagram is adorable but doesn‚Äôt always accurately reflect real-world scenarios. Often, various job titles are stand-ins for an FSDS role. At times, there are hybrid roles, combining DS with MLE or DE with DS. (When you encounter a job post that seems to merge DE and MLE, it often implies you‚Äôre expected to be proficient in DS as well, essentially making it an FSDS role.) So, when roles are well-defined and functionally separate within a company, what can one expect from these overlapping skills/activities?</p> <ul> <li>DE \(\cap\) DS \(\cap\) MLE: <ul> <li><strong>Data Quality</strong>: No matter your title, maintaining data quality is paramount. The age-old saying, ‚Äúgarbage in, garbage out,‚Äù holds across all steps of the data lifecycle. The difference lies in <em>how</em> each role interacts with this asset. Data Engineers may set up validation pipelines, Data Scientists might focus on outlier detection and feature selection, while Machine Learning Engineers ensure that the data feeding into models is squeaky-clean.</li> <li><strong>Domain Expertise</strong>: Technological expertise is essential, but nothing defeats domain-specific knowledge. Whether it‚Äôs identifying which features to build for customer engagement or understanding the regulatory landscape, domain expertise offers a competitive edge that is often overlooked.</li> </ul> </li> <li>DS \(\cap\) ML: <ul> <li><strong>Agile in ML/DS Projects</strong>: Implementing Agile methodologies in this context is filled with challenges. The traditional Agile paradigm is designed to incrementally answer, ‚ÄúIs this software delivering value?‚Äù However, ML/DS projects often revolve around the question, ‚ÄúIs such a model feasible?‚Äù Iterative, incremental improvements and a flexible scope are crucial, but organizations might struggle to understand and budget the open-ended question that is modeling. For example, serving a classification model might change due to performance metrics, if a model doesn‚Äôt reach some predefined threshold it might be used as an internal productivity tool for human labelers instead of an automatic labeling software.</li> </ul> </li> </ul> <h2 id="23-what-roles-can-learn-from-each-other">2.3. What Roles Can Learn From Each Other</h2> <p>The lines may be blurred, but that doesn‚Äôt mean these roles can‚Äôt benefit from each other‚Äôs specialized skill sets:</p> <ul> <li> <p><strong>Data Scientists</strong> should invest more in understanding code quality and software engineering practices. Models are code, after all, and poor code can undermine even the most statistically robust model. As DevOps tries to bridge the gap between developers and operations people, MLOps tries to bridge the gap between DS and MLE.</p> </li> <li> <p><strong>Data Engineers</strong> need to appreciate the nuances of analytics and machine learning models. You‚Äôre not just constructing pipelines; you‚Äôre laying the groundwork for intelligent decision-making.</p> </li> <li> <p><strong>Machine Learning Engineers</strong> ought to prioritize data quality and have an intimate understanding of domain-specific needs. A model is only as good as the data it‚Äôs trained on and the problem it‚Äôs solving.</p> </li> </ul> <p>In closing this section, we can affirm that roles in the data realm are still fluid. The key takeaway is that you shouldn‚Äôt just box yourself into one title or set of responsibilities. Understanding the core competencies of each role not only makes you adaptable but also more valuable in this ever-evolving landscape.</p> <h1 id="3-engineering-vs-data-science-in-companies">3. Engineering vs. Data Science in Companies</h1> <p>In most organizations, engineering roles often get the short end of the stick, relegated to the status of ‚Äúpersona non grata.‚Äù Let‚Äôs not mince words: working as an engineer is frequently an ‚Äúungrateful‚Äù job. While the glory often goes to data scientists for their analytical insights or new ML models, engineers are usually brought into the spotlight only when things go wrong. An uptime of 99.8% won‚Äôt make headlines, but half an hour of downtime will certainly draw ire.</p> <p>This is not just an anecdotal observation; it‚Äôs indicative of an older industry trend. A rapid look at old job postings reveals an excessive focus on Data Science roles, often at the expense of Engineering positions. This dynamic mirrors the longstanding divide between frontend and backend webdev roles, where the former is often considered more relevant due to its direct interaction with users.</p> <p>Nowadays there is an unmistakable shift towards acknowledging the importance of data processes in informed decision-making. Data, after all, is king‚Äîmodels are its subjects, not its rulers. Companies are gradually discovering that <strong>‚Äúgarbage in, company out‚Äù</strong>, a well-oiled data pipeline can be a true goldmine.</p> <p>Even simple models like linear regression, when backed by high-quality data and domain expertise, can substantially empower organizations. The utility of data isn‚Äôt restricted to just complex machine learning models; sometimes, the most effective solutions are also the simplest.</p> <p>In summary, although engineering roles have been historically underappreciated, the scenario is changing. The market is undergoing a correction, emphasizing the equilibrium needed between DS, DE, and MLE. The division of labor is not a zero-sum game; each role brings unique value to the table. Those who adapt to this shift will not just survive but thrive in the data-driven landscape that‚Äôs rapidly taking shape.</p> <p>It‚Äôs heartening to note that many mature companies, and even some startups led by data-savvy founders, are adopting an ‚Äúengineering-aware‚Äù strategy. They understand that for a data project to be successful, it doesn‚Äôt suffice to have just a cutting-edge model; you also need robust data pipelines, efficient data storage solutions, and a reliable deployment strategy.</p> <h1 id="4-conclusion">4. Conclusion</h1> <p>This post began as an endeavor to de-mystify the often blurry lines between DE, DS, and MLE roles. Through a deep dive into role complexity, the intersecting responsibilities, and the underappreciated but crucial importance of engineering roles, we‚Äôve mapped out the complex landscape of the data domain.</p> <p>The data world is in a state of constant change, thanks to ever-evolving technologies and shifting organizational priorities. However, what remains constant is the crucial need for a balanced approach that encompasses Data Science and Engineering capabilities. The underappreciation of engineering roles is beginning to recede, and we‚Äôre witnessing a market correction that places equal emphasis on all pillars of the data domain.</p> <p>Some pragmatic takeaways:</p> <ul> <li>For Professionals: Don‚Äôt pigeonhole yourself into one role; adaptability is your most valuable asset in the tech industry.</li> <li>For Employers: It‚Äôs imperative to understand that a well-oiled data organization needs a balanced team. Invest not just in ‚Äòsexy‚Äô models but also in robust engineering that ensures data quality and robust operation.</li> </ul> <p>The data realm is complex, nuanced, and dynamic. As we move further into this data-centric era, those who can marry analytical rigor with engineering excellence will not just survive but flourish.</p> <h1 id="5-tldr">5. TLDR</h1> <p>Navigating a career in the data domain is complex, with overlapping responsibilities across Data Engineering, Data Science, and Machine Learning Engineering roles. This overlap is more pronounced in startups but exists even in mature companies. The industry focus is shifting‚Äîmoving away from the ‚Äòmodel hype‚Äô to a more balanced outlook that includes robust engineering.</p> <p>Key points to remember:</p> <ul> <li>Role overlap is often a necessity, especially in startups and small companies.</li> <li>Data quality is universally vital across all roles.</li> <li>Agile methodologies pose unique challenges in ML/DS projects.</li> <li>The industry is slowly moving from a Data Science-centric focus to a balanced approach, incorporating more Data Engineering and Machine Learning Engineering roles.</li> </ul> <p>Working in engineering may seem like an ‚Äúungrateful‚Äù job, but those who adapt to the evolving demands will find themselves well-equipped to succeed in this data-driven age.</p>]]></content><author><name></name></author><category term="Career"/><summary type="html"><![CDATA[Thoughts after being a D. Engineer, D. Scientist, and ML Engineer]]></summary></entry><entry><title type="html">Dealing with data in ML projects that go to Production</title><link href="https://romulodrumond.com/blog/2022/dealing-with-data-in-ml-projects-that-go-to-production/" rel="alternate" type="text/html" title="Dealing with data in ML projects that go to Production"/><published>2022-12-05T04:00:00+00:00</published><updated>2022-12-05T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2022/dealing-with-data-in-ml-projects-that-go-to-production</id><content type="html" xml:base="https://romulodrumond.com/blog/2022/dealing-with-data-in-ml-projects-that-go-to-production/"><![CDATA[<ul> <li><a href="#1-introduction">1. Introduction</a></li> <li><a href="#2-do-you-need-data-version-control">2. Do you need data version control?</a></li> <li><a href="#3-tools-for-dealing-with-data-in-ml-projects">3. Tools for dealing with data in ML projects</a> <ul> <li><a href="#31-git-git-lfs">3.1 Git, Git-LFS</a></li> <li><a href="#32-data-version-control-dvc">3.2 Data Version Control (DVC)</a></li> <li><a href="#33-delta-tables-databricks">3.3 Delta Tables (Databricks)</a></li> <li><a href="#34-honorable-mentions">3.4 Honorable mentions</a></li> </ul> </li> <li><a href="#4-conclusion">4. Conclusion</a></li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>‚ÄúData‚Äù in a Machine Learning (ML) project can mean many things:</p> <ul> <li>The train/validation/test data needed to train/validate/test a machine learning model;</li> <li>A special/proprietary tokenizer for Natural Language Processing (NLP) projects;</li> <li>A not-so-small map between entities/variables;</li> <li>An ‚Äúauxiliary‚Äù model, usually static, as the ones used for generating embeds, object detection, etc;</li> <li>and more‚Ä¶</li> </ul> <p>In this blog post ‚Äúdata‚Äù will have this loose definition: any file/artifact that usually is not produced by the developer, having a very wide range of sizes (from KB in case of small data sets to GB in case of big NLP models).</p> <p>With that in mind, a Data Scientist (DS) or Machine Learning Engineer (MLE) should have a solution to store, version, and deploy the data. In an enterprise setting these concerns becomes the questions:</p> <ol> <li>Where is the data stored? How can we configure the access control?</li> <li>Can we have version control? Do we even need that?</li> <li>Deployment to Production and syncs with the Development environment. Do we need environments synced?</li> </ol> <p style="margin-bottom:0;"> <details><summary>(click to expand) <strong>Suggestion when starting</strong></summary> When working on an ML project beyond a Proof of Concept (POC), consider building a list of all data dependencies (not only the external ones). </details> </p> <h1 id="2-do-you-need-data-version-control">2. Do you need data version control?</h1> <p>From the questions raised in the introduction, you may be thinking: <em>Do I even need such a thing as version control on all my data?</em></p> <p>The not-so-surprising answer is: <strong>It depends.</strong></p> <p>What is the motivation for data version control then?</p> <ol> <li>Incident recovery and rollbacks;</li> <li>Model training and data pipelines reproducibility;</li> <li>Bonus: doing a historic data analysis going into the past.</li> </ol> <p>Ok ok, it seems like good capabilities to have, at least for the second point, a crucial one from the ML operations (MLOps) point of view. But having it for all data maybe be troublesome and add no real value. So, <strong>what are the cases in that version control is not necessary?</strong></p> <ul> <li>If the data doesn‚Äôt change frequently or doesn‚Äôt change at all;</li> <li>If new data is only appended, not deleted or updated;</li> <li>If the data changes frequently and the updates make more sense being modeled (e.g. receiving events from external sources that it is not reliable, it makes more sense to have all events in a table with their timestamp)</li> </ul> <h1 id="3-tools-for-dealing-with-data-in-ml-projects">3. Tools for dealing with data in ML projects</h1> <p>With the objective of answering the 3 starting questions, some of the following tools you may consider.</p> <h2 id="31-git-git-lfs">3.1 Git, Git-LFS</h2> <ol> <li><strong>Where is the data stored?</strong> On the GIT remote servers, being external providers (e.g. <a href="https://github.com/">Github</a>, <a href="https://about.gitlab.com/">Gitlab</a>, <a href="https://gitbucket.github.io/">Gitbucket</a>, <a href="https://bitbucket.org/product">Bitbucket</a>‚Ä¶) or self-provided.</li> <li><strong>Can we have version control?</strong> Very strong and familiar version control for developers.</li> <li><strong>Deployment and sync between environments?</strong> Given most projects relies on container images, having the data with the code is the easiest way to deploy it between environments, just needs to deploy the container image.</li> </ol> <p>Other points to consider:</p> <ul> <li>:white_check_mark: Strong versioning and very difficult history deletion;</li> <li>:white_check_mark: Data format agnostic;</li> <li>:x: Container image size: depending on the data size the Continous Delivery (CD) pipelines will slow down and might break due to disk space usage;</li> <li>:x: Default data comparison: you will need to download both file versions and compare if the format is not human-readable. Examples of human-readable: CSV and JSON. Examples of not human-readable: binary and parquet;</li> <li>:x: Max file size of 100 MB on default git and 10 GB on git-lfs.</li> </ul> <h2 id="32-data-version-control-dvc">3.2 Data Version Control (<a href="https://dvc.org/">DVC</a>)</h2> <ol> <li><strong>Where is the data stored?</strong> It accepts many <a href="https://dvc.org/doc/command-reference/remote/add#supported-storage-types">backends</a> (this is pretty awesome by the way).</li> <li><strong>Can we have version control?</strong> Yes, very git-like.</li> <li><strong>Deployment and sync between environments?</strong> Just like git you may do a ‚Äú<code class="language-plaintext highlighter-rouge">git pull</code>‚Äù with a flag.</li> </ol> <p>Other points to consider:</p> <ul> <li>:white_check_mark: Git-like interface;</li> <li>:white_check_mark: Data format agnostic;</li> <li>:white_check_mark: Ability to use multiple backends;</li> </ul> <p><em>Personal note: It is a great tool for migrating from the limitations of Git and Git-LFS, but not so good for big data environments.</em></p> <h2 id="33-delta-tables-databricks">3.3 Delta Tables (Databricks)</h2> <ol> <li><strong>Where is the data stored?</strong> Multiple backends, usually some data lake (ADLS, S3, etc).</li> <li><strong>Can we have version control?</strong> Yes, delta history.</li> <li><strong>Deployment and sync between environments?</strong> Needs to be implemented.</li> </ol> <p>Other points to consider:</p> <ul> <li>:white_check_mark: Easy data comparison: For example, one can use in the same SQL query multiple table versions;</li> <li>:white_check_mark: Can configure retain policy and when to run VACUUM commands, so we can control the period of the data history, consequently, its size;</li> <li>:white_check_mark: :x: Better suited to be used with Spark;</li> <li>:x: Easy table deletion history;</li> <li>:x: Fixed data format: tables.</li> </ul> <h2 id="34-honorable-mentions">3.4 Honorable mentions</h2> <p><strong><a href="https://www.mlflow.org/">MLflow</a></strong>: best option when thinking about versioning models;</p> <p><strong><a href="https://www.pachyderm.com/">Pachyderm</a></strong>: core feature is to run and version data-driven pipelines. Seems to want to do many things at once, too convoluted to be used to solve the starting three problem;</p> <p><strong><a href="https://lakefs.io/">LakeFS</a></strong>: versions the whole data lake, seems like a tool for a company‚Äôs data teams.</p> <p><strong><a href="https://www.dolthub.com/">Dolt</a></strong>: a SQL database that feels like a git repository. The problem is that it is a database in itself, too big of a solution.</p> <h1 id="4-conclusion">4. Conclusion</h1> <p>Given my experiences and analysis of this article, my rules of thumb are:</p> <ul> <li> <p>For data below 100 MB use GIT, for bigger than that, although you could use GIT-LFS, it slows down and may break CD pipelines. GIT is robust and time-proven, also, deploying data in environments is automatic as we usually build container images with all files of the code repository.</p> </li> <li> <p>For bigger data, give preference to Delta Tables as we can control table history range and consequently its size. As our team usually works with Spark, DVC does not show to be too much compelling when compared to Delta tables.</p> </li> </ul>]]></content><author><name></name></author><category term="MLOps"/><category term="Spark"/><summary type="html"><![CDATA[Analysis of problems and tools at our disposal]]></summary></entry><entry><title type="html">Fast-distributed inference with Deep Learning models on Spark</title><link href="https://romulodrumond.com/blog/2022/fast-distributed-inference-with-deep-learning-models-on-spark/" rel="alternate" type="text/html" title="Fast-distributed inference with Deep Learning models on Spark"/><published>2022-11-07T04:00:00+00:00</published><updated>2022-11-07T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2022/fast-distributed-inference-with-deep-learning-models-on-spark</id><content type="html" xml:base="https://romulodrumond.com/blog/2022/fast-distributed-inference-with-deep-learning-models-on-spark/"><![CDATA[<p>Machine learning (ML) models are getting bigger and bigger and <a href="https://arxiv.org/abs/2001.08361">this trend doesn‚Äôt seem to be over</a>. As a result of that, has become mainstream in the industry the use of big models, usually referred to as Deep Learning (DL) models. Although a few companies do have enough data and processing power to train DL from scratch, a common practice is to leverage these big models to produce embeddings, which are a kind of vectorial representation of ‚Äúunstructured‚Äù data as text, images, audio, and videos. The embeddings may be the final product/objective of these models as the <a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder (USE)</a> or a by-product of getting the output of the internal layers of a model (eg. <a href="https://arxiv.org/abs/1810.04805">BERT</a> internal layers). See Figure 1.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings-1400.webp"/> <img src="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/extracting-embeddings.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1 - Embeddings generation with USE (the default output of the model) and with BERT (the embeddings are extracted from an internal layer or a combination of them).</figcaption> </figure> <p>Note: See <a href="http://jalammar.github.io/illustrated-bert/#:~:text=BERT%20for%20feature%20extraction">this</a> to know more about using BERT to generate embeddings.</p> <p>The use of these ‚Äúembedders‚Äù is a good practice as it is a way to leverage pre-trained models to produce tabular data to later be fed in traditional ML techniques (eg. logistic regressions, multilayer perceptrons etc). Companies can then leverage their unstructured data to be used for multiple teams and projects, some companies also started using <a href="https://zilliz.com/learn/what-is-vector-database">vector databases</a> (if you prefer the video format, <a href="https://www.youtube.com/watch?v=g2bNHLeKlAg&amp;ab_channel=SouthernDataScienceConference">this is a good high-level intro</a>) as a way to store these embeddings and have easy use of them in search scenarios.</p> <p>In this context, some companies do the batch generation of these embeddings on big data, on a Spark cluster, as in the company I work on.</p> <p>In a nutshell, <a href="https://spark.apache.org/">Spark</a> is a distributed computing engine that is run on clusters. The architecture consists of a Driver node that sends jobs to, possibly, multiple Worker nodes.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview-1400.webp"/> <img src="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/spark-cluster-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"><small><i>Source: https://spark.apache.org/docs/latest/cluster-overview.html</i></small><br/> Figure 1 - Spark Cluster overview.</figcaption> </figure> </div> <p>As DL models usually involve many parallel operations (as matrix multiplications), a GPU can be multiple times faster than a multicore CPU to run inference/embedding generation. In this scenario, our team used a cluster comprised of multiple Worker nodes, each one with a single GPU (see Figure 2).</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration-1400.webp"/> <img src="/assets/img/posts/2022-11-07-fast-distributed-inference-with-deep-learning-models-on-spark/cluster-configuration.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2 - Overview of cluster used.</figcaption> </figure> </div> <p>With the right abstractions, an efficient <a href="https://spark.apache.org/docs/latest/api/python/">PySpark</a> code would be something along these lines:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">generate_embeddings</span><span class="p">(</span><span class="n">embeder_model</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">:</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="n">broacasted_embeddings_model</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">broadcast</span><span class="p">(</span><span class="n">embeder_model</span><span class="p">)</span>

    <span class="nd">@pandas_udf</span><span class="p">(</span><span class="n">returnType</span><span class="o">=</span><span class="nc">ArrayType</span><span class="p">(</span><span class="nc">FloatType</span><span class="p">()))</span>
    <span class="k">def</span> <span class="nf">batch_embed</span><span class="p">(</span><span class="n">dataset</span><span class="p">:</span> <span class="n">pandas</span><span class="p">.</span><span class="n">Series</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pandas</span><span class="p">.</span><span class="n">Series</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">broacasted_embeddings_model</span><span class="p">.</span><span class="n">value</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">toGPU</span><span class="p">()</span>

        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        
        <span class="n">embeddings_cpu</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">embeddings</span> <span class="k">else</span> <span class="p">[]</span>

        <span class="k">return</span> <span class="n">pandas</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">embeddings_cpu</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">dataframe</span><span class="p">.</span><span class="nf">withColumn</span><span class="p">(</span><span class="n">EMBEDDINGS_COLUMN_NAME</span><span class="p">,</span> <span class="nf">batch_embed</span><span class="p">(</span><span class="n">dataframe</span><span class="p">[</span><span class="n">DATA_COLUMN_NAME</span><span class="p">]))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now for you to understand this pseudo-code we have some important notes:</p> <ul> <li> <p><strong>Broadcast of the model</strong><br/> As each worker node needs the model, it has to be sent beforehand. Line 2 broadcasts the model from the driver node and in line 6 a worker node gets the python object back.</p> </li> <li> <p><strong>Why pandas UDF instead of Spark UDF?</strong><br/> Traditional Spark UDFs run row by row of the data frame while pandas UDFs run on batches of the data (multiple rows at once). Trying to run a DL model for each row separately would add unnecessary GPU to CPU communication and wouldn‚Äôt leverage GPU‚Äôs parallel capabilities.</p> </li> </ul> <p>Other important params of your code and Spark cluster might be:</p> <ul> <li><strong>Apache Arrow batch size</strong>: you might need to reduce the default 10,000 value of <code class="language-plaintext highlighter-rouge">spark.sql.execution.arrow.maxRecordsPerBatch</code> if your data is too big (take a look <a href="https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#setting-arrow-batch-size">here</a>);</li> <li><strong>Data batch size for the GPU</strong>: although choosing a power of 2 (eg. 32, 64, 128‚Ä¶) doesn‚Äôt seem to have a real impact, it may help you reduce the search space of this parameter. The clear rule is to try to maximize your GPU memory use without running into Out Of Memory (OOM) errors;</li> <li><strong>GPU choice</strong>: although the number of cores and memory available widely varies between GPUs, your choice may be restricted by what is available from your vendor and the size of the model used.</li> </ul> <p>A final important note is that even if nodes with GPU are more expensive than CPU only, the final cost for running batch prediction (\($ = hourlyCost * hoursNeeded\)) is usually in favor of GPU clusters for DL models.</p> <p>That‚Äôs it! Feel free to reach out if you have doubts or some feedback to give.</p>]]></content><author><name></name></author><category term="Spark"/><category term="Deep Learning"/><summary type="html"><![CDATA[Learn how to efficiently distribute inference load on Spark]]></summary></entry><entry><title type="html">Using Lens to inspect a Kubernetes minikube cluster on WSL</title><link href="https://romulodrumond.com/blog/2022/using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/" rel="alternate" type="text/html" title="Using Lens to inspect a Kubernetes minikube cluster on WSL"/><published>2022-10-31T04:00:00+00:00</published><updated>2022-10-31T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2022/using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl</id><content type="html" xml:base="https://romulodrumond.com/blog/2022/using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/"><![CDATA[<ul> <li><a href="#1-a-bit-of-context">1. A bit of context</a> <ul> <li><a href="#11-lens">1.1 Lens</a></li> <li><a href="#12-minikube">1.2 Minikube</a></li> <li><a href="#13-wsl">1.3 WSL</a></li> </ul> </li> <li><a href="#2-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl">2. Using Lens to inspect a Kubernetes minikube cluster on WSL</a></li> </ul> <h1 id="1-a-bit-of-context">1. A bit of context</h1> <h2 id="11-lens">1.1 Lens</h2> <p><a href="https://k8slens.dev/">Lens</a> is like a <a href="https://kubernetes.io/">Kubernetes (K8s)</a> IDE, with the app you can monitor and interact with K8s clusters using a graphical user interface (GUI). See Video 1 for an intro on that desktop app.</p> <div style="position:relative;padding-bottom:56.25%;"> <iframe style="width:100%;height:100%;position:absolute;left:0px;top:0px;" width="100%" height="100%" src="https://www.youtube.com/embed/eeDwdVXattc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p style="text-align:center; margin-top:0.15cm;"> Video 1 - Introduction to Lens. </p> <p>In the context of this tutorial, we installed Lens on Windows 11.</p> <h2 id="12-minikube">1.2 Minikube</h2> <p><a href="https://minikube.sigs.k8s.io/docs/start/">Minikube</a> is an easy way to have a local K8s cluster running on your computer, all you need is Docker container or some similar tool, and <a href="https://kubernetes.io/docs/reference/kubectl/kubectl/">kubectl</a>. For a quick intro to this tool check Video 2.</p> <div style="position:relative;padding-bottom:56.25%;"> <iframe style="width:100%;height:100%;position:absolute;left:0px;top:0px;" width="100%" height="100%" src="https://www.youtube.com/embed/E2pP1MOfo3g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p style="text-align:center; margin-top:0.15cm;"> Video 2 - Minikube and Kubectl explained. </p> <p>Note: <a href="https://www.youtube.com/c/TechWorldwithNana">TechWorld with Nana</a> is an amazing channel! Check it out if you are interested in DevOps.</p> <p>In the context of this tutorial we installed minikube on WSL, so we followed the Linux installation guide from the <a href="https://minikube.sigs.k8s.io/docs/start/">minikube website</a>.</p> <p>After running the <code class="language-plaintext highlighter-rouge">minikube start</code> command you can check if you are using Docker, the containers that represent your K8s with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>‚ûú  ~ docker ps
CONTAINER ID   IMAGE                                 COMMAND                  CREATED       STATUS         PORTS                                                                                                                                  NAMES
1536bce200c   gcr.io/k8s-minikube/kicbase:v0.0.33   <span class="s2">"/usr/local/bin/entr‚Ä¶"</span>   2 weeks ago   Up 2 minutes   127.0.0.1:49157-&gt;22/tcp, 127.0.0.1:49156-&gt;2376/tcp, 127.0.0.1:49155-&gt;5000/tcp, 127.0.0.1:49154-&gt;8443/tcp, 127.0.0.1:49153-&gt;32443/tcp   minikube
</code></pre></div></div> <p>And with kubectl you can check the nodes too:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>‚ûú  ~ kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   20d   v1.24.3
</code></pre></div></div> <p>You may want to change your kubectl context if you have multiple clusters configured, for example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>‚ûú  ~ kubectl config get-contexts <span class="c"># get available contexts</span>
CURRENT   NAME                         CLUSTER                      AUTHINFO                                                       NAMESPACE
          minikube                     minikube                     minikube                                                       default
<span class="k">*</span>         other-cluster                other-cluster                clusterUser_k8s_other-cluster               
‚ûú  ~ kubectl config use-context minikube <span class="c"># in this example, changes the context from 'other-cluster' to 'minikube'</span>
Switched to context <span class="s2">"minikube"</span><span class="nb">.</span>
</code></pre></div></div> <p>Also, you may simulate a multi-node K8s instance with minikube if you want to test a multi-node deployment (when we talk about databases, it is a good practice to have at least 3 instances in different nodes for robustness to node failure). You can create a new multi-node cluster/profile by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minikube start <span class="nt">--nodes</span> &lt;number-of-nodes&gt; <span class="nt">--profile</span> &lt;name-of-new-profile&gt;
</code></pre></div></div> <p style="margin-bottom:0;"> <details><summary>(click to expand) <strong>Example run</strong></summary> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">‚ûú  ~ minikube start <span class="nt">--nodes</span> 3 <span class="nt">--profile</span> multinode3
üòÑ  <span class="o">[</span>multinode3] minikube v1.27.0 on Ubuntu 20.04 <span class="o">(</span>amd64<span class="o">)</span>
‚ùó  Kubernetes 1.25.0 has a known issue with resolv.conf. minikube is using a workaround that should work <span class="k">for </span>most use cases.
‚ùó  For more information, see: https://github.com/kubernetes/kubernetes/issues/112135
‚ú®  Automatically selected the docker driver
üìå  Using Docker driver with root privileges
üëç  Starting control plane node multinode3 <span class="k">in </span>cluster multinode3
üöú  Pulling base image ...
üî•  Creating docker container <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB<span class="o">)</span> ...
üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI <span class="o">(</span>Container Networking Interface<span class="o">)</span> ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node multinode3-m02 <span class="k">in </span>cluster multinode3
üöú  Pulling base image ...
üî•  Creating docker container <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB<span class="o">)</span> ...
üåê  Found network options:
    ‚ñ™ <span class="nv">NO_PROXY</span><span class="o">=</span>192.153.27.2
üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
    ‚ñ™ <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>192.153.27.2
üîé  Verifying Kubernetes components...

üëç  Starting worker node multinode3-m03 <span class="k">in </span>cluster multinode3
üöú  Pulling base image ...
üî•  Creating docker container <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB<span class="o">)</span> ...
üåê  Found network options:
    ‚ñ™ <span class="nv">NO_PROXY</span><span class="o">=</span>192.153.27.2,192.153.27.3
üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
    ‚ñ™ <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>192.153.27.2
    ‚ñ™ <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>192.153.27.2,192.168.27.3
üîé  Verifying Kubernetes components...
üèÑ  Done! kubectl is now configured to use <span class="s2">"multinode3"</span> cluster and <span class="s2">"default"</span> namespace by default</code></pre></figure> </details> </p> <p style="margin-bottom:0;"> You may check your multiple profiles with <code>minikube profile list</code>, change profile with <code>minikube profile &lt;profile-name&gt;</code>, and run specific actions to profiles by adding <code>-p &lt;profile-name&gt;</code> to the end of the command. <details><summary>(click to expand) <strong>Example run</strong></summary> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">‚ûú  ~ minikube profile list                     
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
|  Profile   | VM Driver | Runtime |      IP      | Port | Version | Status  | Nodes | Active |
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
| minikube   | docker    | docker  | 192.153.49.2 | 8443 | v1.24.3 | Running |     1 | <span class="k">*</span>      |
| multinode3 | docker    | docker  | 192.153.85.2 | 8443 | v1.25.0 | Running |     3 |        |
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
‚ûú  ~ minikube stop <span class="nt">-p</span> minikube <span class="c"># stoping the cluster of the 'minikube' profile</span>
‚úã  Stopping node <span class="s2">"minikube"</span>  ...
üõë  Powering off <span class="s2">"minikube"</span> via SSH ...
üõë  1 node stopped.
‚ûú  ~ minikube profile multinode3 <span class="c"># changing current profile</span>
‚úÖ  minikube profile was successfully <span class="nb">set </span>to multinode3
‚ûú  ~ minikube profile list      
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
|  Profile   | VM Driver | Runtime |      IP      | Port | Version | Status  | Nodes | Active |
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
| minikube   | docker    | docker  | 192.153.49.2 | 8443 | v1.24.3 | Stopped |     1 |        |
| multinode3 | docker    | docker  | 192.153.85.2 | 8443 | v1.25.0 | Running |     3 | <span class="k">*</span>      |
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|</code></pre></figure> </details> </p> <p style="margin-bottom:0;"> And you may see your <code>kubectl</code> available contexts with the command <code>kubectl config get-contexts</code> and take a look at the new nodes with <code>kubectl get nodes</code>. <details><summary>(click to expand) <strong>Example run</strong></summary> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">‚ûú  ~ kubectl config get-contexts 
CURRENT   NAME                         CLUSTER                      AUTHINFO                                                       NAMESPACE
          minikube                     minikube                     minikube                                                       default
<span class="k">*</span>         multinode3                   multinode3                   multinode3                                                     default
          other-cluster                other-cluster                clusterUser_k8s_other-cluster
‚ûú  ~ kubectl get nodes
NAME             STATUS   ROLES           AGE     VERSION
multinode3       Ready    control-plane   6m27s   v1.25.0
multinode3-m02   Ready    &lt;none&gt;          4m30s   v1.25.0
multinode3-m03   Ready    &lt;none&gt;          3m53s   v1.25.0</code></pre></figure> </details> </p> <p>Note: Although is possible to run this multi-node test locally it is difficult to work on a modest machine now that we have 3 containers each expecting 2 CPUs and 2 Gb of RAM :disappointed:.</p> <h2 id="13-wsl">1.3 WSL</h2> <p>I‚Äôve made a quick intro about it on an <a href="/blog/2022/setting-up-docker-on-wsl2/">older post</a>, but you can go straight to this quick <a href="https://www.youtube.com/watch?v=MrZolfGm8Zk&amp;ab_channel=MicrosoftDeveloper">video intro</a>. Basically, WSL is a tool to have a Linux developer environment on your Windows PC.</p> <hr/> <h1 id="2-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl">2. Using Lens to inspect a Kubernetes minikube cluster on WSL</h1> <p><em>Wait! Couldn‚Äôt I just use <code class="language-plaintext highlighter-rouge">minikube dashboard</code> instead?</em></p> <p>Yes, minikube comes with a dashboard (see Figure 1) so you can look up your local K8s cluster and interact with it, but <strong>the idea of using Lens is to have the same tool for interacting with the local cluster and with a production cluster in a cloud provider</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-dashboard-print-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-dashboard-print-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-dashboard-print-1400.webp"/> <img src="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-dashboard-print.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1 - Minikube's dashboard example.</figcaption> </figure> <p style="margin-bottom:0;"> First, check if your cluster is running with <code>minikube status</code> command, if no cluster is running you can spin one up with the <code>minikube start</code> command. <details><summary>(click to expand) <strong>Example run</strong></summary> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">‚ûú  ~ minikube status
multinode3
<span class="nb">type</span>: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

multinode3-m02
<span class="nb">type</span>: Worker
host: Running
kubelet: Running

multinode3-m03
<span class="nb">type</span>: Worker
host: Running
kubelet: Running</code></pre></figure> </details> </p> <p>If you still don‚Äôt have it, download and install <a href="https://k8slens.dev/">Lens</a>. Start it and go the the ‚ÄúCluster‚Äù tab as shown by the left arrow in Figure 2 and then click on the plus sign pointed by the right arrow. Figure 3 shows the possible ways to add clusters to Lens.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/lens-initial-screen-for-setup-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/lens-initial-screen-for-setup-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/lens-initial-screen-for-setup-1400.webp"/> <img src="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/lens-initial-screen-for-setup.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2 - Cluster tab on Lens.</figcaption> </figure> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/sync-kube-config-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/sync-kube-config-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/sync-kube-config-1400.webp"/> <img src="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/sync-kube-config.png" class="img-fluid rounded z-depth-1" width="30%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 3 - Lens options for adding clusters.</figcaption> </figure> </div> <p>Here comes the problem, you may want to sync the kubeconfig file that you can find in a path like this: <code class="language-plaintext highlighter-rouge">\\wsl.localhost\Ubuntu-20.04\home\romulo\.kube\config</code> when using the first option shown in Figure 3. You shall end up with a list of clusters similar to Figure 4, where in my case I can easily access the second one, a K8s cluster on Azure. The problem lies when trying to access the minikube one, the error in Figure 5 is raised.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/available-clusters-after-sync-kubeconfig-file-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/available-clusters-after-sync-kubeconfig-file-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/available-clusters-after-sync-kubeconfig-file-1400.webp"/> <img src="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/available-clusters-after-sync-kubeconfig-file.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 4 - Lens list of available clusters after syncing kubeconfig file.</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-connection-error-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-connection-error-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-connection-error-1400.webp"/> <img src="/assets/img/posts/2022-10-31-using-lens-to-inspect-a-kubernetes-minikube-cluster-on-wsl/minikube-connection-error.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 5 - Error while trying to access a minikube cluster with Lens.</figcaption> </figure> <details><summary>(click to expand) <strong>Full error</strong></summary> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">F1027 17:47:49.413105 54440 main.go:74] failed to initialize kubeconfiginvalid configuration: <span class="o">[</span>unable to <span class="nb">read </span>client-cert <span class="se">\\</span>wsl.localhost<span class="se">\U</span>buntu-20.04<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>kube<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>minikube<span class="se">\p</span>rofiles<span class="se">\m</span>ultinode3<span class="se">\c</span>lient.crt <span class="k">for </span>multinode3 due to open <span class="se">\\</span>wsl.localhost<span class="se">\U</span>buntu-20.04<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>kube<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>minikube<span class="se">\p</span>rofiles<span class="se">\m</span>ultinode3<span class="se">\c</span>lient.crt: The system cannot find the path specified., unable to <span class="nb">read </span>client-key <span class="se">\\</span>wsl.localhost<span class="se">\U</span>buntu-20.04<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>kube<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>minikube<span class="se">\p</span>rofiles<span class="se">\m</span>ultinode3<span class="se">\c</span>lient.key <span class="k">for </span>multinode3 due to open <span class="se">\\</span>wsl.localhost<span class="se">\U</span>buntu-20.04<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>kube<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>minikube<span class="se">\p</span>rofiles<span class="se">\m</span>ultinode3<span class="se">\c</span>lient.key: The system cannot find the path specified., unable to <span class="nb">read </span>certificate-authority <span class="se">\\</span>wsl.localhost<span class="se">\U</span>buntu-20.04<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>kube<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>minikube<span class="se">\c</span>a.crt <span class="k">for </span>multinode3 due to open <span class="se">\\</span>wsl.localhost<span class="se">\U</span>buntu-20.04<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>kube<span class="se">\h</span>ome<span class="se">\r</span>omulo<span class="se">\.</span>minikube<span class="se">\c</span>a.crt: The system cannot find the path specified.]
goroutine 1 <span class="o">[</span>running]:
k8s.io/klog/v2.stacks<span class="o">(</span>0x1<span class="o">)</span>
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:1140 +0x8a
k8s.io/klog/v2.<span class="o">(</span><span class="k">*</span>loggingT<span class="o">)</span>.output<span class="o">(</span>0x22a5140, 0x3, 0x0, 0xc0001810a0, 0x1, <span class="o">{</span>0x1a7c0b5, 0x20<span class="o">}</span>, 0x22a6020, 0x0<span class="o">)</span>
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:1088 +0x66f
k8s.io/klog/v2.<span class="o">(</span><span class="k">*</span>loggingT<span class="o">)</span>.printDepth<span class="o">(</span>0xc000720800, 0x6018, 0x0, <span class="o">{</span>0x0, 0x0<span class="o">}</span>, 0x1, <span class="o">{</span>0xc000386100, 0x2, 0x2<span class="o">})</span>
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:735 +0x1ae
k8s.io/klog/v2.<span class="o">(</span><span class="k">*</span>loggingT<span class="o">)</span>.print<span class="o">(</span>...<span class="o">)</span>
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:717
k8s.io/klog/v2.Fatal<span class="o">(</span>...<span class="o">)</span>
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:1622
main.main<span class="o">()</span>
D:/a/lens-k8s-proxy/lens-k8s-proxy/main.go:74 +0x6a6

goroutine 6 <span class="o">[</span>chan receive]:
k8s.io/klog/v2.<span class="o">(</span><span class="k">*</span>loggingT<span class="o">)</span>.flushDaemon<span class="o">(</span>0x0<span class="o">)</span>
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:1283 +0x6a
created by k8s.io/klog/v2.init.0
C:/Users/runneradmin/go/pkg/mod/k8s.io/klog/v2@v2.40.1/klog.go:420 +0xfb

goroutine 10 <span class="o">[</span>syscall]:
os/signal.signal_recv<span class="o">()</span>
C:/hostedtoolcache/windows/go/1.17.8/x64/src/runtime/sigqueue.go:169 +0x98
os/signal.loop<span class="o">()</span>
C:/hostedtoolcache/windows/go/1.17.8/x64/src/os/signal/signal_unix.go:24 +0x19
created by os/signal.Notify.func1.1
C:/hostedtoolcache/windows/go/1.17.8/x64/src/os/signal/signal.go:151 +0x2c

proxy exited with code: 255

Failed to start connection: Error: failed to retrieve port from stream</code></pre></figure> </details> <p><br/></p> <p>From the error log we can easily see that part of the path is being repeated, e.g. <code class="language-plaintext highlighter-rouge">\home\romulo</code> in <code class="language-plaintext highlighter-rouge">\\wsl.localhost\Ubuntu-20.04\home\romulo\.kube\home\romulo\.minikube\profiles\multinode3\client.crt</code>.</p> <p>So, the best approach to deal with this mismatch I‚Äôve found is to make a copy of the <code class="language-plaintext highlighter-rouge">~/.kube/config</code> file to, for example, <code class="language-plaintext highlighter-rouge">~/kubeconfig</code> and delete on this new file all instances of <code class="language-plaintext highlighter-rouge">/home/&lt;username&gt;/</code> (in my case <code class="language-plaintext highlighter-rouge">/home/romulo/</code>). This way you don‚Äôt mess around with the config file necessary for <code class="language-plaintext highlighter-rouge">minikube</code> and <code class="language-plaintext highlighter-rouge">kubectl</code> to work properly while you use Lens.</p> <p>After syncing this new <code class="language-plaintext highlighter-rouge">~/kubeconfig</code> file with Lens, you will still need to update it if your <code class="language-plaintext highlighter-rouge">~/.kube/config</code> changes (possibly by adding/deleting clusters connections). Nevertheless minikube has an annoying behavior: it changes the server port every time the cluster restarts. For example, you may find at the first minikube start a line in the kubeconfig file with <code class="language-plaintext highlighter-rouge">server: https://127.0.0.1:49174</code> and then, after a cluster restart you may see <code class="language-plaintext highlighter-rouge">server: https://127.0.0.1:49189</code>. Fortunately, if you are using Docker or Podman to virtualize the cluster, you can set up the port forwarding on the creation of the minikube cluster/profile. You shall run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minikube delete <span class="c"># to delete old docker containers for the active profile</span>
minikube start <span class="nt">--ports</span><span class="o">=</span>127.0.0.1:55555:8443 <span class="c"># forwarding &lt;server-port-you-want-fixed&gt;:&lt;K8s-default-apiserver-port&gt;</span>
</code></pre></div></div> <details><summary>(click to expand) <strong>Example run:</strong></summary> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">‚ûú  ~ minikube delete <span class="c"># to delete old docker containers for the active profile</span>
üî•  Deleting <span class="s2">"minikube"</span> <span class="k">in </span>docker ...
üî•  Deleting container <span class="s2">"minikube"</span> ...
üî•  Removing /home/romulo/.minikube/machines/minikube ...
üíÄ  Removed all traces of the <span class="s2">"minikube"</span> cluster.
‚ûú  ~ minikube start <span class="nt">--ports</span><span class="o">=</span>127.0.0.1:55555:8443 <span class="c"># forwarding &lt;server-port-you-want-fixed&gt;:&lt;K8s-default-apiserver-port&gt;</span>
üòÑ  minikube v1.27.0 on Ubuntu 20.04 <span class="o">(</span>amd64<span class="o">)</span>
‚ùó  Kubernetes 1.25.0 has a known issue with resolv.conf. minikube is using a workaround that should work <span class="k">for </span>most use cases.
‚ùó  For more information, see: https://github.com/kubernetes/kubernetes/issues/112135
‚ú®  Automatically selected the docker driver
üìå  Using Docker driver with root privileges
üëç  Starting control plane node minikube <span class="k">in </span>cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB<span class="o">)</span> ...
üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass
üèÑ  Done! kubectl is now configured to use <span class="s2">"minikube"</span> cluster and <span class="s2">"default"</span> namespace by default</code></pre></figure> </details> <p>Note: The <code class="language-plaintext highlighter-rouge">55555</code> is an arbitrary port number I‚Äôve chosen (it‚Äôs five fives :smile:) and <code class="language-plaintext highlighter-rouge">8443</code> is the default API server port for K8s, if you change it you shall change it here too.</p> <p>Unfortunately, this port fixing trick doesn‚Äôt work with multi-node minikube cluster (see <a href="https://github.com/kubernetes/minikube/pull/9404#issuecomment-1192735773">PR#9404</a>). If you want to see for yourself try to create a new cluster with <code class="language-plaintext highlighter-rouge">minikube start --nodes 3 --profile multinode3 --ports=127.0.0.1:55555:8443</code> or add a new node to the single node cluster created before with <code class="language-plaintext highlighter-rouge">minikube node add</code>. Both of them raise an error due to trying to use the same port forwarding (<code class="language-plaintext highlighter-rouge">127.0.0.1:55555:8443</code>) for the worker nodes too. <strong>For the case of a multi-node minikube cluster you will need to keep updating the server port every time you want to use Lens and the cluster has been restarted</strong> :disappointed_relieved:.</p> <p>I personally maintain a single node profile named <em>minikube</em> with the port forwarding trick and a multi-node one named <em>multinode3</em>. When I want to use Lens with the multi-node one I manually update my <code class="language-plaintext highlighter-rouge">~/kubeconfig</code> file with the new random port.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>‚ûú  ~ minikube profile list
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
|  Profile   | VM Driver | Runtime |      IP      | Port | Version | Status  | Nodes | Active |
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
| minikube   | docker    | docker  | 192.153.49.2 | 8443 | v1.25.0 | Stopped |     1 | <span class="k">*</span>      |
| multinode3 | docker    | docker  | 192.153.67.2 | 8443 | v1.25.0 | Stopped |     3 |        |
|------------|-----------|---------|--------------|------|---------|---------|-------|--------|
</code></pre></div></div>]]></content><author><name></name></author><category term="Kubernetes"/><category term="WSL"/><summary type="html"><![CDATA[Learn how to use Lens to see K8s clusters on WSL]]></summary></entry><entry><title type="html">Setting up Docker on WSL2</title><link href="https://romulodrumond.com/blog/2022/setting-up-docker-on-wsl2/" rel="alternate" type="text/html" title="Setting up Docker on WSL2"/><published>2022-08-29T04:00:00+00:00</published><updated>2022-08-29T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2022/setting-up-docker-on-wsl2</id><content type="html" xml:base="https://romulodrumond.com/blog/2022/setting-up-docker-on-wsl2/"><![CDATA[<ul> <li><a href="#1-a-bit-of-context">1. A bit of context</a></li> <li><a href="#2-setting-up-docker-on-wsl2">2. Setting up Docker on WSL2</a></li> <li><a href="#3-bonus">3. Bonus</a></li> </ul> <h1 id="1-a-bit-of-context">1. A bit of context</h1> <p>So, it has been a journey to use a Windows 11 machine again and leave Linux behind (used mostly Ubuntu in my dev/machine learning experience). Although in the beginning, the Windows Subsystem for Linux (WSL) had many problems, today it is in a useable state (all the engineering team I work on uses Windows with WSL and we all develop in ‚ÄúLinux‚Äù). If you don‚Äôt know the tool or maybe dismissed it in the past I advise you to give it a try, just go to the <a href="https://docs.microsoft.com/en-us/windows/wsl/about">Microsoft docs</a>. PS: Today you can even lunch Linux applications that have UI (<a href="https://docs.microsoft.com/en-us/windows/wsl/tutorials/gui-apps">link</a>).</p> <div style="position:relative;padding-bottom:56.25%;"> <iframe style="width:100%;height:100%;position:absolute;left:0px;top:0px;" width="100%" height="100%" src="https://www.youtube.com/embed/MrZolfGm8Zk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p style="text-align:center; margin-top:0.15cm;"> Video 1: A good intro to the capabilities of WSL2 with VSCode. </p> <p>In the development world, using containers (see Video 2) is usually the go-to standard for delivering applications, not just that but when deploying and training machine learning models too as we can use container images to spin up Spark clusters (see Video 3) and Dask clusters (see Video 4). With that in mind, a famous engine used for containers is <a href="https://www.docker.com/">Docker</a>, which we can use to build and run container images and test our applications. With that in mind, for a development environment to work properly on WSL we need an engine, and we will be using Docker for it.</p> <div style="position:relative;padding-bottom:56.25%;"> <iframe style="width:100%;height:100%;position:absolute;left:0px;top:0px;" width="100%" height="100%" src="https://www.youtube.com/embed/0qotVMX-J5s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p style="text-align:center; margin-top:0.15cm;"> Video 2: Containerization Explained. </p> <div style="position:relative;padding-bottom:56.25%;"> <iframe style="width:100%;height:100%;position:absolute;left:0px;top:0px;" width="100%" height="100%" src="https://www.youtube.com/embed/ymtq8yjmD9I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p style="text-align:center; margin-top:0.15cm;"> Video 3: A good intro to Spark. </p> <div style="position:relative;padding-bottom:56.25%;"> <iframe style="width:100%;height:100%;position:absolute;left:0px;top:0px;" width="100%" height="100%" src="https://www.youtube.com/embed/nnndxbr_Xq4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p style="text-align:center; margin-top:0.15cm;"> Video 4: A good intro to Dask. </p> <h1 id="2-setting-up-docker-on-wsl2">2. Setting up Docker on WSL2</h1> <p>If you can/plan to use <a href="https://www.docker.com/products/docker-desktop/">Docker Desktop</a> on Windows to manage your containers, it is pretty straightforward, just follow this <a href="https://docs.docker.com/desktop/windows/wsl/">tutorial</a>. But, be aware that Docker Desktop is changing its <a href="https://docs.docker.com/subscription/#docker-desktop-license-agreement">license agreement</a> and you become paid for large organizations, although the docker engine will remain free.</p> <p>So, if you want to remain using docker for free you can install the Docker engine and Docker compose as you would in a Linux machine (links below).</p> <p><a href="https://docs.docker.com/engine/install/">How to install Docker Engine/CLI</a></p> <p><a href="https://docs.docker.com/compose/install/">How To install docker-compose</a></p> <p>The problem of WSL will appear while trying to run a docker image. You would get an error like that:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run hello-world 
docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See <span class="s1">'docker run --help'</span><span class="nb">.</span>
</code></pre></div></div> <p>For docker to properly work on WSL you will need to start the service with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>service docker start
</code></pre></div></div> <p>Then docker should work fine. But it would suck for every time WSL restarts you need to run the sudo command. What can you do? You can update the <code class="language-plaintext highlighter-rouge">/etc/wsl.conf</code> file on your Linux distribution to run the boring command at startup. As an example:</p> <div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># /etc/wsl.conf
</span>[<span class="n">boot</span>]
<span class="n">command</span>=<span class="s2">"service docker start"</span>
</code></pre></div></div> <p>That‚Äôs it! Docker will be ready to run every time you start WSL.</p> <h1 id="3-bonus">3. Bonus</h1> <p>You can add more commands to start services at WSL startup, as an example, you could add cron:</p> <div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># /etc/wsl.conf
</span>[<span class="n">boot</span>]
<span class="n">command</span>=<span class="s2">"service docker start; service cron start"</span>
</code></pre></div></div> <p>For more configuration options go to <a href="https://docs.microsoft.com/en-us/windows/wsl/wsl-config">Microsoft‚Äôs doc page</a>.</p>]]></content><author><name></name></author><category term="WSL"/><summary type="html"><![CDATA[Quick tutorial to ease up using Docker on WSL without Docker Desktop]]></summary></entry><entry><title type="html">Welcome! This is my first post</title><link href="https://romulodrumond.com/blog/2022/welcome-post/" rel="alternate" type="text/html" title="Welcome! This is my first post"/><published>2022-04-30T04:00:00+00:00</published><updated>2022-04-30T04:00:00+00:00</updated><id>https://romulodrumond.com/blog/2022/welcome-post</id><content type="html" xml:base="https://romulodrumond.com/blog/2022/welcome-post/"><![CDATA[<p>Hi there! Welcome to my blog!</p> <p>I finally had the energy (and time) to launch my website, and with it, I‚Äôm launching a blog! So, what will it be about? Mostly technical stuff I‚Äôm interested in and maybe some career thoughts/advice, but other topics may show up, who knows?</p> <p>As I enumerated some of my interests on the <a href="https://romulodrumond.com">homepage</a> you may know what to expect. In the near future content about <a href="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</a> will probably appear as I‚Äôm studying the subject with some friends (I may try some applications in the financial market as I still have the illusion that one day I‚Äôll be rich using math :disappointed:).</p> <p>That‚Äôs it! I hope to see you soon. If this post wasn‚Äôt much useful for you I present you a pretty useful software:</p> <iframe width="100%" height="500" src="https://pointerpointer.com/" title="Point finder" frameborder="0" allowfullscreen=""></iframe> <p>Source: <a href="https://pointerpointer.com/">https://pointerpointer.com/</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Just a short welcome post]]></summary></entry></feed>